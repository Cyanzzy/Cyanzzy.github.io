---
title: DL-MULI-4-自动微分
date: 2023-09-09 09:20:43
tags: 
  - DL
categories: 
  - Science
password: zzy   
message: 亲，能不能输入密码啊？
---
# 向量链式法则

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-24.png)

> 例1

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-25.png)

> 例2

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-26.png)

# 自动求导

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-27.png)

> 自动求导

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-29.png)

> 反向累积

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-30.png)

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-31.png)

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-32.png)

# 计算图

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-28.png)

# 简单栗子

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-129.png)

 在我们计算 $y$ 关于 $x$ 的梯度之前，需要一个地方来存储梯度。 重要的是，我们不会在每次对一个参数求导时都分配新的内存。 

 一个标量函数关于向量 $x$ 的梯度是向量，并且与 $x$ 具有相同的形状。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-130.png)

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-131.png)

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-132.png)

# 非标量变量的反向传播

 当`y`不是标量时，向量`y`关于向量`x`的导数的最自然解释是一个矩阵。 对于高阶和高维的`y`和`x`，求导的结果可以是一个高阶张量。 

 当调用向量的反向计算时，我们通常会试图计算一批训练样本中每个组成部分的损失函数的导数。 这里，我们的目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-133.png)

# 分离计算

有时，我们希望将某些计算移动到记录的计算图之外。 例如，假设`y`是作为`x`的函数计算的，而`z`则是作为`y`和`x`的函数计算的。 想象一下，我们想计算`z`关于`x`的梯度，但由于某种原因，希望将`y`视为一个常数， 并且只考虑到`x`在`y`被计算后发挥的作用。

这里可以分离`y`来返回一个新变量`u`，该变量与`y`具有相同的值， 但丢弃计算图中如何计算`y`的任何信息。 换句话说，梯度不会向后流经`u`到`x`。 因此，下面的反向传播函数计算`z=u*x`关于`x`的偏导数，同时将`u`作为常数处理， 而不是`z=x*x*x`关于`x`的偏导数。

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-134.png)

#  Python控制流的梯度计算

 使用自动微分的一个好处是： 即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度。 

在下面的代码中，`while`循环的迭代次数和`if`语句的结果都取决于输入`a`的值。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-135.png)

