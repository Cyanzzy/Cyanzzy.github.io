---
title: DL-MULI-6-线性神经网络
date: 2023-09-09 13:13:11
tags: 
  - DL
categories: 
  - Science
---

# 线性回归

## 线性回归的基本元素

> 线性模型

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-141.png)

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-33.png)

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-34.png)

> 损失函数

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-142.png)

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-143.png)

> 显示解

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-37.png)

> 梯度下降

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-38.png)

选择学习率

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-39.png)

> 小批量随机梯度下降

 梯度下降最简单的用法是计算损失函数（数据集中所有样本的损失均值） 关于模型参数的导数（在这里也可以称为梯度）。 但实际中的执行可能会非常慢：因为在每一次更新参数之前，我们必须遍历整个数据集。 因此，我们通常会在每次需要计算更新的时候随机抽取一小批样本， 这种变体叫做`小批量随机梯度下降`

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-144.png)

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-40.png)

**选择批量**

不能太小：每次计算量太小不适合并行来最大利用计算资源

不能太大：内存消耗增加浪费计算

**总结**

* 梯度下降通过不断沿着反梯度方向更新参数求解
* 小批量随机梯度下降是深度学习默认的求解算法
* 两个重要的超参数是批量大小和学习率

> 用模型进行预测

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-145.png)



## 矢量化加速

 在训练我们的模型时，我们经常希望能够同时处理整个小批量的样本。 为了实现这一点，需要我们对计算进行矢量化， 从而利用线性代数库，而不是在Python中编写开销高昂的for循环。 

## 正态分布与平方损失

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-146.png)

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-147.png)

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-148.png)

# 线性回归从零实现

## 生成数据集

 为了简单起见，我们将根据带有噪声的线性模型构造一个人造数据集。 我们的任务是使用这个有限样本的数据集来恢复这个模型的参数。 我们将使用低维数据，这样可以很容易地将其可视化。 

  在下面的代码中，我们生成一个包含1000个样本的数据集， 每个样本包含从标准正态分布中采样的2个特征。  

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-149.png)

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-150.png)

 `features`中每一行都包含一个二维数据样本，`labels`中的每一行都包含一维标签值（一个标量）

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-151.png)

 通过生成第二个特征`features[:, 1]`和`labels`的散点图， 可以直观观察到两者之间的线性关系。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-152.png)

## 读取数据集

 在下面的代码中，我们定义一个`data_iter`函数， 该函数接收批量大小、特征矩阵和标签向量作为输入，生成大小为`batch_size`的小批量。 每个小批量包含一组特征和标签 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-153.png)

我们直观感受一下小批量运算：读取第一个小批量数据样本并打印。 每个批量的特征维度显示批量大小和输入特征数。 同样的，批量的标签形状与`batch_size`相等。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-154.png)

## 初始化模型参数

 在我们开始用小批量随机梯度下降优化我们的模型参数之前， 我们需要先有一些参数。 在下面的代码中，我们通过从均值为0、标准差为0.01的正态分布中采样随机数来初始化权重， 并将偏置初始化为0。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-155.png)

 在初始化参数之后，我们的任务是更新这些参数，直到这些参数足够拟合我们的数据。 每次更新都需要计算损失函数关于模型参数的梯度。 有了这个梯度，我们就可以向减小损失的方向更新每个参数。  

## 定义模型

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-156.png)



## 定义损失函数

 因为需要计算损失函数的梯度，所以我们应该先定义损失函数。  在实现中，我们需要将真实值`y`的形状转换为和预测值`y_hat`的形状相同。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-157.png)



## 定义优化算法

 在每一步中，使用从数据集中随机抽取的一个小批量，然后根据参数计算损失的梯度。 接下来，朝着减少损失的方向更新我们的参数。  

 下面的函数实现小批量随机梯度下降更新。 该函数接受模型参数集合、学习速率和批量大小作为输入。每 一步更新的大小由学习速率`lr`决定。 因为我们计算的损失是一个批量样本的总和，所以我们用批量大小（`batch_size`） 来规范化步长，这样步长大小就不会取决于我们对批量大小的选择。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-158.png)



## 训练过程

 在每次迭代中，我们读取一小批量训练样本，并通过我们的模型来获得一组预测。 计算完损失后，我们开始反向传播，存储每个参数的梯度。 最后，我们调用优化算法`sgd`来更新模型参数。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-159.png)



 在每个`迭代周期`（epoch）中，我们使用`data_iter`函数遍历整个数据集， 并将训练数据集中所有样本都使用一次（假设样本数能够被批量大小整除）。 这里的迭代周期个数`num_epochs`和学习率`lr`都是超参数，分别设为3和0.03。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-160.png)

   因为我们使用的是自己合成的数据集，所以我们知道真正的参数是什么。 因此，我们可以通过比较真实参数和通过训练学到的参数来评估训练的成功程度。 事实上，真实参数和通过训练学到的参数确实非常接近。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-161.png)

# 线性回归简洁实现

## 生成数据集

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-162.png)

## 读取数据集

 我们可以调用框架中现有的API来读取数据。 我们将`features`和`labels`作为API的参数传递，并通过数据迭代器指定`batch_size`。 此外，布尔值`is_train`表示是否希望数据迭代器对象在每个迭代周期内打乱数据。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-163.png)

 为了验证是否正常工作，让我们读取并打印第一个小批量样本。 这里我们使用`iter`构造Python迭代器，并使用`next`从迭代器中获取第一项。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-164.png)

## 定义模型

 对于标准深度学习模型，我们可以使用框架的预定义好的层。这使我们只需关注使用哪些层来构造模型，而不必关注层的实现细节。

 我们首先定义一个模型变量`net`，它是一个`Sequential`类的实例。 `Sequential`类将多个层串联在一起。 当给定输入数据时，`Sequential`实例将数据传入到第一层， 然后将第一层的输出作为第二层的输入，以此类推。

 在下面的例子中，我们的模型只包含一个层，因此实际上不需要`Sequential`。 但是由于以后几乎所有的模型都是多层的，在这里使用`Sequential`会让你熟悉“标准的流水线”。 

 回顾单层网络架构， 这一单层被称为`全连接层`（fully-connected layer）， 因为它的每一个输入都通过矩阵-向量乘法得到它的每个输出。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-165.png)

在PyTorch中，全连接层在`Linear`类中定义。 值得注意的是，我们将两个参数传递到`nn.Linear`中。 第一个指定输入特征形状，即2，第二个指定输出特征形状，输出特征形状为单个标量，因此为1 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-166.png)

## 初始化模型参数

 在使用`net`之前，我们需要初始化模型参数。 如在线性回归模型中的权重和偏置。 深度学习框架通常有预定义的方法来初始化参数。 

在这里，我们指定每个权重参数应该从均值为0、标准差为0.01的正态分布中随机采样， 偏置参数将初始化为零。 

 正如我们在构造`nn.Linear`时指定输入和输出尺寸一样， 现在我们能直接访问参数以设定它们的初始值。 我们通过`net[0]`选择网络中的第一个图层， 然后使用`weight.data`和`bias.data`方法访问参数。 我们还可以使用替换方法`normal_`和`fill_`来重写参数值。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-167.png)

## 定义损失函数

 计算均方误差使用的是`MSELoss`类，也称为平方 $L_2$ 范数。 默认情况下，它返回所有样本损失的平均值。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-168.png)

## 定义优化算法

 小批量随机梯度下降算法是一种优化神经网络的标准工具， PyTorch在`optim`模块中实现了该算法的许多变种。 

当我们实例化一个`SGD`实例时，我们要指定优化的参数 （可通过`net.parameters()`从我们的模型中获得）以及优化算法所需的超参数字典。 小批量随机梯度下降只需要设置`lr`值，这里设置为0.03。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-169.png)

## 训练过程

在每个迭代周期里，我们将完整遍历一次数据集（`train_data`）， 不停地从中获取一个小批量的输入和相应的标签。 对于每一个小批量，我们会进行以下步骤：

- 通过调用`net(X)`生成预测并计算损失`l`（前向传播）。

- 通过进行反向传播来计算梯度。

- 通过调用优化器来更新模型参数。

  为了更好的衡量训练效果，我们计算每个迭代周期后的损失，并打印它来监控训练过程。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-170.png)

 下面我们比较生成数据集的真实参数和通过有限数据训练获得的模型参数。 要访问参数，我们首先从`net`访问所需的层，然后读取该层的权重和偏置 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-171.png)

# Softmax回归

## 回归与分类

* 回归预估连续值
* 分类预测离散类别

> 回归

* 单连续数值输出
* 自然区间$\R$
* 跟真实值的区别作为损失

> 分类

* 通常多个输出
* 输出i是预测为第i类的置信度

### 从回归到多分类

### 均方损失

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-41.png)

### 无校验比例

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-42.png)

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-43.png)

## 分类问题

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-173.png)

## 网络架构

 为了估计所有可能类别的条件概率，我们需要一个有多个输出的模型，每个类别对应一个输出。 为了解决线性模型的分类问题，我们需要和输出一样多的*仿射函数*（affine function）。 每个输出对应于它自己的仿射函数。 

假设有4个特征和3个可能的输出类别， 我们将需要12个标量来表示权重（带下标的 $w$）， 3个标量来表示偏置（带下标的 $b$） 

 下面我们为每个输入计算三个未规范化的预测

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-172.png)

 为了更简洁地表达模型，我们仍然使用线性代数符号。 通过向量形式表达为 $o=Wx+b$， 这是一种更适合数学和编写代码的形式。 由此，我们已经将所有权重放到一个 $3×4$ 矩阵中。 对于给定数据样本的特征 $x$， 我们的输出是由权重与输入特征进行矩阵-向量乘法再加上偏置 $b$ 得到的。 

## softmax运算

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-175.png)

 softmax函数能够将未规范化的预测变换为非负数并且总和为1，同时让模型保持 可导的性质。 为了完成这一目标，我们首先对每个未规范化的预测求幂，这样可以确保输出非负。 为了确保最终输出的概率值总和为1，我们再让每个求幂后的结果除以它们的总和 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-174.png)

 尽管softmax是一个非线性函数，但softmax回归的输出仍然由输入特征的仿射变换决定。 因此，softmax回归是一个`线性模型`

## 小批量样本的矢量化

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-176.png)

## 损失函数

 我们需要一个损失函数来度量预测的效果，将使用最大似然估计 

> 对数似然

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-177.png)

> softmax及其导数

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-178.png)

> 交叉熵损失

## ![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-44.png)信息论基础

`信息论`（information theory）涉及编码、解码、发送以及尽可能简洁地处理信息或数据。 

> 熵

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-179.png)

> 信息量

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-180.png)

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-181.png)

##  模型预测和评估

 在训练softmax回归模型后，给出任何样本特征，我们可以预测每个输出类别的概率。 通常我们使用预测概率最高的类别作为输出类别。 如果预测与实际类别（标签）一致，则预测是正确的。 在接下来的实验中，我们将使用`精度`（accuracy）来评估模型的性能。 精度等于正确预测数与预测总数之间的比率。 

## 小结

* softmax运算获取一个向量并将其映射为概率。
* softmax回归适用于分类问题，它使用了softmax运算中输出类别的概率分布。
* 交叉熵是一个衡量两个概率分布之间差异的很好的度量，它测量给定模型编码数据所需的比特数

# 常用损失函数

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-45.png)

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-46.png)

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-47.png)

# Softmax从零实现

使用Fashion-MNIST数据集， 并设置数据迭代器的批量大小为256。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-182.png)

## 初始化模型参数

原始数据集中的每个样本都是28×28的图像，我们将展平每个图像，将它们视为长度为784的向量。因为我们的数据集有10个类别，所以网络输出维度为10

因此，权重将构成一个784×10的矩阵， 偏置将构成一个1×10的行向量。 与线性回归一样，我们将使用正态分布初始化我们的权重`W`，偏置初始化为0。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-183.png)

## 定义softmax操作

 给定一个矩阵`X`，我们可以对所有元素求和（默认情况下）。 也可以只求同一个轴上的元素，即同一列（轴0）或同一行（轴1）。 

如果`X`是一个形状为`(2, 3)`的张量，我们对列进行求和， 则结果将是一个具有形状`(3,)`的向量。 当调用`sum`运算符时，我们可以指定保持在原始张量的轴数，而不折叠求和的维度。 这将产生一个具有形状`(1, 3)`的二维张量。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-184.png)

实现softmax由三个步骤组成：

1. 对每个项求幂（使用`exp`）；
2. 对每一行求和（小批量中每个样本是一行），得到每个样本的规范化常数；
3. 将每一行除以其规范化常数，确保结果的和为1。

$$
softmax(X)_{ij}=\frac{exp(X_{ij})}{\sum_{k}^{}{exp(X_{ik})}}
$$

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-185.png)

 正如上述代码，对于任何随机输入，我们将每个元素变成一个非负数。 此外，依据概率原理，每行总和为1。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-186.png)

 注意，虽然这在数学上看起来是正确的，但我们在代码实现中有点草率。 矩阵中的非常大或非常小的元素可能造成数值上溢或下溢，但我们没有采取措施来防止这点。 

## 定义模型

 定义softmax操作后，我们可以实现softmax回归模型。 下面的代码定义了输入如何通过网络映射到输出。 注意，将数据传递到模型之前，我们使用`reshape`函数将每张原始图像展平为向量 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-187.png)

## 定义损失函数

下面，我们创建一个数据样本`y_hat`，其中包含2个样本在3个类别的预测概率， 以及它们对应的标签`y`。 有了`y`，我们知道在第一个样本中，第一类是正确的预测； 而在第二个样本中，第三类是正确的预测。 然后使用`y`作为`y_hat`中概率的索引， 我们选择第一个样本中第一个类的概率和第二个样本中第三个类的概率。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-188.png)

实现交叉熵损失函数

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-189.png)

## 分类精度

 给定预测概率分布`y_hat`，当我们必须输出硬预测（hard prediction）时， 我们通常选择预测概率最高的类。  当预测与标签分类`y`一致时，即是正确的。 分类精度即正确预测数量与总预测数量之比。 

 为了计算精度，我们执行以下操作。 首先，如果`y_hat`是矩阵，那么假定第二个维度存储每个类的预测分数。 我们使用`argmax`获得每行中最大元素的索引来获得预测类别。 然后我们将预测类别与真实`y`元素进行比较。 由于等式运算符“`==`”对数据类型很敏感， 因此我们将`y_hat`的数据类型转换为与`y`的数据类型一致。 结果是一个包含0（错）和1（对）的张量。 最后，我们求和会得到正确预测的数量。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-190.png)

 我们将继续使用之前定义的变量`y_hat`和`y`分别作为预测的概率分布和标签。 可以看到，第一个样本的预测类别是2（该行的最大元素为0.6，索引为2），这与实际标签0不一致。 第二个样本的预测类别是2（该行的最大元素为0.5，索引为2），这与实际标签2一致。 因此，这两个样本的分类精度率为0.5。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-191.png)

 同样，对于任意数据迭代器`data_iter`可访问的数据集， 我们可以评估在任意模型`net`的精度。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-192.png)

 这里定义一个实用程序类`Accumulator`，用于对多个变量进行累加。 在上面的`evaluate_accuracy`函数中， 我们在`Accumulator`实例中创建了2个变量， 分别用于存储正确预测的数量和预测的总数量。 当我们遍历数据集时，两者都将随着时间的推移而累加。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-193.png)

 由于我们使用随机权重初始化`net`模型， 因此该模型的精度应接近于随机猜测。 例如在有10个类别情况下的精度为0.1。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-194.png)

## 训练过程

 首先，我们定义一个函数来训练一个迭代周期。 请注意，`updater`是更新模型参数的常用函数，它接受批量大小作为参数。 它可以是`d2l.sgd`函数，也可以是框架的内置优化函数。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-195.png)

 在展示训练函数的实现之前，我们定义一个在动画中绘制数据的实用程序类`Animator`， 它能够简化本书其余部分的代码。 

```python
class Animator:  #@save
    """在动画中绘制数据"""
    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,
                 ylim=None, xscale='linear', yscale='linear',
                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,
                 figsize=(3.5, 2.5)):
        # 增量地绘制多条线
        if legend is None:
            legend = []
        d2l.use_svg_display()
        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)
        if nrows * ncols == 1:
            self.axes = [self.axes, ]
        # 使用lambda函数捕获参数
        self.config_axes = lambda: d2l.set_axes(
            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)
        self.X, self.Y, self.fmts = None, None, fmts

    def add(self, x, y):
        # 向图表中添加多个数据点
        if not hasattr(y, "__len__"):
            y = [y]
        n = len(y)
        if not hasattr(x, "__len__"):
            x = [x] * n
        if not self.X:
            self.X = [[] for _ in range(n)]
        if not self.Y:
            self.Y = [[] for _ in range(n)]
        for i, (a, b) in enumerate(zip(x, y)):
            if a is not None and b is not None:
                self.X[i].append(a)
                self.Y[i].append(b)
        self.axes[0].cla()
        for x, y, fmt in zip(self.X, self.Y, self.fmts):
            self.axes[0].plot(x, y, fmt)
        self.config_axes()
        display.display(self.fig)
        display.clear_output(wait=True)
```

 接下来我们实现一个训练函数， 它会在`train_iter`访问到的训练数据集上训练一个模型`net`。 该训练函数将会运行多个迭代周期（由`num_epochs`指定）。 在每个迭代周期结束时，利用`test_iter`访问到的测试数据集对模型进行评估。 我们将利用`Animator`类来可视化训练进度。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-196.png)

 作为一个从零开始的实现，使用小批量随机梯度下降来优化模型的损失函数，设置学习率为0.1。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-197.png)

 现在，我们训练模型10个迭代周期。 请注意，迭代周期（`num_epochs`）和学习率（`lr`）都是可调节的超参数。 通过更改它们的值，我们可以提高模型的分类精度。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-198.png)



## 图像分类预测

 现在训练已经完成，我们的模型已经准备好对图像进行分类预测。 给定一系列图像，我们将比较它们的实际标签（文本输出的第一行）和模型预测（文本输出的第二行）。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-199.png)

- 借助softmax回归，我们可以训练多分类的模型。
- 训练softmax回归循环模型与训练线性回归模型非常相似：先读取数据，再定义模型和损失函数，然后使用优化算法训练模型。大多数常见的深度学习模型都有类似的训练过程。

# Softmax简洁实现

 继续使用Fashion-MNIST数据集，并保持批量大小为256。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-200.png)

## 初始化模型参数

  softmax回归的输出层是一个全连接层。 因此，为了实现我们的模型， 我们只需在`Sequential`中添加一个带有10个输出的全连接层。 同样，在这里`Sequential`并不是必要的， 但它是实现深度模型的基础。 我们仍然以均值0和标准差0.01随机初始化权重。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-201.png)

## 重新审视Softmax的实现

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-202.png)

 我们也希望保留传统的softmax函数，以备我们需要评估通过模型输出的概率。 但是，我们没有将softmax概率传递到损失函数中， 而是在交叉熵损失函数中传递未规范化的预测，并同时计算softmax及其对数 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-203.png)

## 优化算法

 我们使用学习率为0.1的小批量随机梯度下降作为优化算法 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-204.png)

## 训练过程

 我们调用定义的训练函数来训练模型。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-205.png)