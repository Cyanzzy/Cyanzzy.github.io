---
title: DL-MULI-7-多层感知机
date: 2023-09-10 13:15:18
tags: 
  - DL
categories: 
  - Science
---


# 感知机

> 感知机

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-52.png)

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-53.png)

> 收敛定理

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-54.png)

> XOR问题

感知机不能拟合XOR函数，它只能产生线性分割面

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-55.png)

> 总结

* 感知机是一个二分类模型
* 它的求解算法等价于使用批量大小为1的梯度下降
* 它不能拟合XOR函数

# 多层感知机

> 学习XOR

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-56.png)

## 隐藏层

### 网络中加入隐藏层

 将许多全连接层堆叠在一起。 每一层都输出到上面的层，直到生成最后的输出。 我们可以把前 $L-1$ 层看作表示，把最后一层看作线性预测器。 这种架构通常称为`多层感知机`（multilayer perceptron），通常缩写为*MLP*。  

> 这个多层感知机有4个输入，3个输出，其隐藏层包含5个隐藏单元。 输入层不涉及任何计算，因此使用此网络产生输出只需要实现隐藏层和输出层的计算。 因此，这个多层感知机中的层数为2。 注意，这两个层都是全连接的。 每个输入都会影响隐藏层中的每个神经元， 而隐藏层中的每个神经元又会影响输出层中的每个神经元。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-57.png)

> 单分类

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-58.png)

> 多类分类

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-62.png)

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-63.png)

> 多隐藏层

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-64.png)



### 从线性到非线性

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-206.png)

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-207.png)

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-208.png)

## 激活函数

`激活函数`（activation function）通过计算加权和并加上偏置来确定神经元是否应该被激活， 它们将输入信号转换为输出的可微运算。 大多数激活函数都是非线性的。 

### sigmoid激活函数

 对于一个定义域在 $\R$ 中的输入， `sigmoid函数`将输入变换为区间 $(0, 1)$ 上的输出。 因此，sigmoid通常称为*挤压函数*（squashing function）： 它将范围（-inf, inf）中的任意输入压缩到区间（0, 1）中的某个值： 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-59.png)

 ![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-209.png)

### Tanh激活函数

 与sigmoid函数类似， tanh(双曲正切)函数也能将其输入压缩转换到区间(-1, 1)上。 tanh函数的公式如下： 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-60.png)

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-210.png)

### ReLU激活函数

最受欢迎的激活函数是`修正线性单元`（Rectified linear unit，*ReLU*）， 因为它实现简单，同时在各种预测任务中表现良好。 ReLU提供了一种非常简单的非线性变换。 给定元素$x$，ReLU函数被定义为该元素与 $0$ 的最大值： 

 通俗地说，ReLU函数通过将相应的活性值设为0，仅保留正元素并丢弃所有负元素。 为了直观感受一下，我们可以画出函数的曲线图。 正如从图中所看到，激活函数是分段线性的。 


![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-61.png)



 使用ReLU的原因是，它求导表现得特别好：要么让参数消失，要么让参数通过。 这使得优化表现得更好，并且ReLU减轻了困扰以往神经网络的梯度消失问题。 



## 总结

* 多层感知机使用隐藏层和激活函数来得到非线性模型
* 常用的激活函数是Sigmoid、Tanh、ReLU
* 使用softmax来处理多类分类
* 超参数为隐藏层数，和各个隐藏层大小

# 多层感知机从零实现

 我们将继续使用Fashion-MNIST图像分类数据集 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-211.png)

## 初始化模型参数

 Fashion-MNIST中的每个图像由 28×28=784个灰度像素值组成。 所有图像共分为10个类别。 忽略像素之间的空间结构， 我们可以将每个图像视为具有784个输入特征 和10个类的简单分类数据集 

 首先，我们将实现一个具有单隐藏层的多层感知机， 它包含256个隐藏单元。 注意，我们可以将这两个变量都视为超参数。

 通常，我们选择2的若干次幂作为层的宽度。 因为内存在硬件中的分配和寻址方式，这么做往往可以在计算上更高效。 

 我们用几个张量来表示我们的参数。 注意，对于每一层我们都要记录一个权重矩阵和一个偏置向量。 跟以前一样，我们要为损失关于这些参数的梯度分配内存。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-212.png)

## 激活函数

 为了确保我们对模型的细节了如指掌， 我们将实现ReLU激活函数， 而不是直接调用内置的`relu`函数。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-213.png)

## 模型

 因为我们忽略了空间结构， 所以我们使用`reshape`将每个二维图像转换为一个长度为`num_inputs`的向量。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-214.png)

## 损失函数

在这里我们直接使用高级API中的内置函数来计算softmax和交叉熵损失。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-215.png)

## 训练过程

 多层感知机的训练过程与softmax回归的训练过程完全相同。 可以直接调用`d2l`包的`train_ch3`函数， 将迭代周期数设置为10，并将学习率设置为0.1. 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-216.png)

 为了对学习到的模型进行评估，我们将在一些测试数据上应用这个模型。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-217.png)

# 多层感知机简洁实现

与softmax回归的简洁实现相比， 唯一的区别是我们添加了2个全连接层（之前我们只添加了1个全连接层）。 第一层是隐藏层，它包含256个隐藏单元，并使用了ReLU激活函数。 第二层是输出层。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-218.png)

 训练过程的实现与我们实现softmax回归时完全相同， 这种模块化设计使我们能够将与模型架构有关的内容独立出来。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-219.png)



# 模型选择、过拟合和欠拟合

## 训练误差和泛化误差

训练误差：模型在训练数据上的误差

泛化误差：模型在新数据上的误差

## 模型选择

> 验证数据集和测试数据集

验证数据集：一个用来评估模型好坏的数据集

测试数据集：只用一次的数据集

> K-则交叉验证

* 将训练数据分割成K块

* $For \ i = 1, ..., K$

  使用第 $i$ 块作为验证数据集，其余的作为训练数据集

* 报告K个验证集误差的平均

* 常用：K=5或10

> 小结

* 训练数据集：训练模型参数
* 验证数据集：选择模型超参数
* 非大数据集上通常使用k-折交叉验证

## 欠拟合和过拟合

> **欠拟合（Underfitting）**：

- **定义**：欠拟合指的是模型对训练数据和新数据的拟合都不足够好。换句话说，模型无法捕捉到数据中的真实模式或趋势。
- **特征**：欠拟合的模型通常在训练数据和测试数据上都表现较差。它们在训练数据上的误差和测试数据上的误差都比较高。
- **原因**：欠拟合通常发生在模型过于简单或者没有足够的训练数据时。如果模型复杂度太低，它可能无法适应数据中的复杂关系。

> **过拟合（Overfitting）**：

- **定义**：过拟合指的是模型在训练数据上表现得非常好，但在新数据上表现不佳。模型过于复杂，它学到了训练数据中的噪声和随机变化，而不是真正的模式。
- **特征**：过拟合的模型在训练数据上表现出色，但在测试数据上的性能往往差得多。在测试数据上的误差远高于在训练数据上的误差。
- **原因**：过拟合通常发生在模型过于复杂或者训练数据不足时。如果模型的容量过大，它可能会记住训练数据中的每一个细节，而不是学习通用的规律。

> **欠拟合的应对**：

- 增加模型的复杂度：可以尝试使用更复杂的模型，例如增加神经网络的层数或增加决策树的深度。
- 添加更多的特征：如果模型无法捕捉到数据的关键特征，可以考虑添加更多有意义的特征。
- 增加训练数据：更多的数据可以帮助模型更好地学习数据中的模式。

> **过拟合的应对**：

- 减小模型的复杂度：可以通过减少神经网络的层数、减小决策树的深度或减少模型的参数数量来降低复杂度。
- 正则化：使用正则化技术如L1正则化或L2正则化，可以限制模型参数的大小，减少过拟合风险。
- 增加训练数据：更多的训练数据有助于模型更好地泛化，减少过拟合。
- 使用交叉验证：通过交叉验证来评估模型的性能，选择最佳的模型参数。

> 模型容量



![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-65.png)



* 拟合各种函数的能力
* 低容量的模型难以拟合训练数据
* 高容量的模型可以记住所有的训练数据

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-66.png)

> 估计模型容量

* 难以在不同种类算法之间比较
* 给定一个模型种类，将有两个主要因素
  * 参数的个数
  * 参数值的选择范围

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-67.png)

# 权重衰减

 在训练参数化机器学习模型时， `权重衰减`（weight decay）是最广泛使用的正则化的技术之一， 它通常也被称为 $L_2$正则化。 这项技术通过函数与零的距离来衡量函数的复杂度， 因为在所有函数 $f$ 中，函数 $f=0$（所有输入都得到值0） 在某种意义上是最简单的 

 一种简单的方法是通过线性函数 $f(x)=w^Tx$ 中的权重向量的某个范数来度量其复杂性， 例如 $||w||^2$。 **要保证权重向量比较小， 最常用方法是将其范数作为惩罚项加到最小化损失的问题中**。 将原来的训练目标`最小化训练标签上的预测损失`， 调整为`最小化预测损失和惩罚项之和`。  

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-220.png)

 对于 $\lambda = 0$，我们恢复了原来的损失函数。 对于$\lambda>0$ ，我们限制 $‖w‖$ 的大小。 这里我们仍然除以2：当我们取一个二次函数的导数时， 2和1/2会抵消，以确保更新表达式看起来既漂亮又简单 

> 为什么我们首先使用 $L_2$ 范数，而不是 $L_1$ 范数 

$L_2$ 正则化线性模型构成经典的`岭回归`（ridge regression）算法，$L_1$ 正则化线性回归是统计学中类似的基本模型， 通常被称为`套索回归`（lasso regression）。 

使用 $L_2$ 范数的一个原因是它对权重向量的大分量施加了巨大的惩罚。 这使得我们的学习算法**偏向于在大量特征上均匀分布权重的模型**。 在实践中，这可能使它们对单个变量中的观测误差更为稳定。 

相比之下，$L_1$ 惩罚会导致模型将权重集中在一小部分特征上， 而将其他权重清除为零。 这称为`特征选择` 

 $L_2$ 正则化回归的小批量随机梯度下降更新如下式： 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-221.png)

 我们根据估计值与观测值之间的差异来更新 $w$。 然而，我们同时也在试图将 $w$ 的大小缩小到零。 这就是为什么这种方法有时被称为*权重衰减*。 我们仅考虑惩罚项，优化算法在训练的每一步`衰减`权重。 与特征选择相比，权重衰减为我们提供了一种连续的机制来调整函数的复杂度。 较小的 $\lambda$ 值对应较少约束的 $w$ ， 而较大的 $\lambda$ 值对 $w$ 的约束更大。 

## 高维线性回归

 我们通过一个简单的例子来演示权重衰减。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-222.png)

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-226.png)



## 从零开始实现

 下面我们将从头开始实现权重衰减，只需将  $L_2$  的平方惩罚添加到原始目标函数中。 

### 初始化模型参数

 首先，我们将定义一个函数来随机初始化模型参数。

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-223.png) 

### 定义 $L2$ 范数惩罚

 实现这一惩罚最方便的方法是对所有项求平方后并将它们求和。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-224.png)

### 定义训练代码

 下面的代码将模型拟合训练数据集，并在测试数据集上进行评估 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-225.png)

### 忽略正则化直接训练

 我们现在用`lambd = 0`禁用权重衰减后运行这个代码。 注意，这里训练误差有了减少，但测试误差没有减少， 这意味着出现了严重的过拟合。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-227.png)

### 使用权重衰减

 下面，我们使用权重衰减来运行代码。 注意，在这里训练误差增大，但测试误差减小。 这正是我们期望从正则化中得到的效果。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-228.png)

## 简洁实现

 由于权重衰减在神经网络优化中很常用， 深度学习框架为了便于我们使用权重衰减， 将权重衰减集成到优化算法中，以便与任何损失函数结合使用。 

此外，这种集成还有计算上的好处， 允许在不增加任何额外的计算开销的情况下向算法中添加权重衰减。 由于更新的权重衰减部分仅依赖于每个参数的当前值， 因此优化器必须至少接触每个参数一次。 

 在下面的代码中，我们在实例化优化器时直接通过`weight_decay`指定weight decay超参数。 默认情况下，PyTorch同时衰减权重和偏移。 这里我们只为权重设置了`weight_decay`，所以偏置参数`b`不会衰减。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-229.png)

 这些图看起来和我们从零开始实现权重衰减时的图相同。 然而，它们运行得更快，更容易实现。 对于更复杂的问题，这一好处将变得更加明显 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-230.png)

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-231.png)

## 小结

- 正则化是处理过拟合的常用方法：在训练集的损失函数中加入惩罚项，以降低学习到的模型的复杂度。
- 保持模型简单的一个特别的选择是使用 $L_2$ 惩罚的权重衰减。这会导致学习算法更新步骤中的权重衰减。
- 权重衰减功能在深度学习框架的优化器中提供。
- 在同一训练代码实现中，不同的参数集可以有不同的更新行为。

# Dropout

**Dropout** 是深度学习中一种常用的正则化技术，旨在减轻神经网络的过拟合问题。Dropout的核心思想是在训练过程中随机地禁用一部分神经元，使网络不依赖于特定的神经元，从而提高泛化性能。以下是有关Dropout的详细说明：

> **工作原理**：

- 在每个训练迭代中，Dropout会随机选择一些神经元，并将它们的输出置为零。这相当于将这些神经元临时从网络中删除。
- 通常，Dropout会应用于隐藏层的神经元，而不是输入层或输出层。
- 每次迭代都会随机选择不同的神经元进行Dropout，以增加网络的多样性。

> **Dropout率**：

- Dropout率是指在每个迭代中随机禁用神经元的概率。典型的Dropout率通常在0.2到0.5之间，但具体的值需要根据问题和网络结构进行调整。
- Dropout率越高，模型的正则化效果越强，但也可能降低训练速度。

> **训练和测试时的不同行为**：

- 在训练期间，Dropout是启用的，以减轻过拟合。在每个训练迭代中，都会应用Dropout。
- 在测试或推理期间，通常会禁用Dropout，以确保模型的预测是确定性的。

> **作用**：

- Dropout强制网络学习到更加鲁棒的特征表示，因为网络不能依赖于特定的神经元。
- 它减少了神经元之间的共适应性，帮助模型更好地泛化到未见过的数据。
- Dropout还可以被视为一种集成学习方法，因为它训练了多个不同的子网络，最终将它们组合起来以进行预测。

> **适用性**：

- Dropout通常适用于深度神经网络，特别是在数据量有限或容易过拟合的情况下。
- 它对卷积神经网络（CNNs）、循环神经网络（RNNs）和全连接神经网络都适用。

> **替代方法**：

- 除了Dropout，还有其他正则化方法，如L1和L2正则化、批标准化等，可以用来减轻过拟合问题。
- 在某些情况下，使用Dropout与其他正则化技术组合可以取得更好的效果。

总之，Dropout是一种有效的正则化技术，可用于改善深度神经网络的泛化能力，减轻过拟合问题。通过在训练期间随机禁用神经元，Dropout促使网络更加鲁棒，降低了依赖于特定神经元的风险。

## 实践中的暂退法

图带有1个隐藏层和5个隐藏单元的多层感知机。 当我们将暂退法应用到隐藏层，以 $p$ 的概率将隐藏单元置为零时， 结果可以看作一个只包含原始神经元子集的网络。 删除 $ℎ_2$ 和 $ℎ_5$ ， 因此输出的计算不再依赖于 $ℎ_2$ 或  $ℎ_5$ ，并且它们各自的梯度在执行反向传播时也会消失。 这样，输出层的计算不能过度依赖于 $ℎ_1,…,ℎ_5$ 的任何一个元素。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-232.png)

 通常，我们在测试时不用暂退法。 给定一个训练好的模型和一个新的样本，我们不会丢弃任何节点，因此不需要标准化。 然而也有一些例外：一些研究人员在测试时使用暂退法， 用于估计神经网络预测的“不确定性”： 如果通过许多不同的暂退法遮盖后得到的预测结果都是一致的，那么我们可以说网络发挥更稳定 

## 从零开始实现

 要实现单层的暂退法函数， 我们从均匀分布 $U[0,1]$ 中抽取样本，样本数与这层神经网络的维度一致。 然后我们保留那些对应样本大于 $p$ 的节点，把剩下的丢弃。 

 在下面的代码中，我们实现 `dropout_layer` 函数， 该函数以`dropout`的概率丢弃张量输入`X`中的元素， 如上所述重新缩放剩余部分：将剩余部分除以`1.0-dropout`。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-233.png)

 我们可以通过下面几个例子来测试`dropout_layer`函数。 我们将输入`X`通过暂退法操作，暂退概率分别为0、0.5和1。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-234.png)

### 定义模型参数

 引入Fashion-MNIST数据集。 我们定义具有两个隐藏层的多层感知机，每个隐藏层包含256个单元。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-235.png)

### 定义模型

 我们可以将暂退法应用于每个隐藏层的输出（在激活函数之后）， 并且可以为每一层分别设置暂退概率： 常见的技巧是在靠近输入层的地方设置较低的暂退概率。 下面的模型将第一个和第二个隐藏层的暂退概率分别设置为0.2和0.5， 并且暂退法只在训练期间有效。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-236.png)

### 训练和测试

 这类似于前面描述的多层感知机训练和测试。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-238.png)

## 简洁实现

 对于深度学习框架的高级API，我们只需在每个全连接层之后添加一个`Dropout`层， 将暂退概率作为唯一的参数传递给它的构造函数。 在训练时，`Dropout`层将根据指定的暂退概率随机丢弃上一层的输出（相当于下一层的输入）。 在测试时，`Dropout`层仅传递数据。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-239.png)



 接下来，我们对模型进行训练和测试。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-240.png)

## 小结

- 暂退法在前向传播过程中，计算每一内部层的同时丢弃一些神经元。
- 暂退法可以避免过拟合，它通常与控制权重向量的维数和大小结合使用的。
- 暂退法将活性值ℎ替换为具有期望值ℎ的随机变量。
- 暂退法仅在训练期间使用。

# 前向传播、反向传播和计算图

## 前向传播

> 前向传播（forward propagation或forward pass）

按顺序（从输入层到输出层）计算和存储神经网络中每层的结果。

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-241.png)

## 前向传播计算图

 绘制`计算图`有助于我们可视化计算中操作符和变量的依赖关系。 下图是与上述简单网络相对应的计算图， 其中正方形表示变量，圆圈表示操作符。 左下角表示输入，右上角表示输出。 注意显示数据流的箭头方向主要是向右和向上的。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-242.png)

## 反向传播

> 反向传播（backward propagation或backpropagation）

计算神经网络参数梯度的方法， 该方法根据微积分中的*链式规则*，按相反的顺序从输出层到输入层遍历网络。 该算法存储了计算某些参数梯度时所需的任何中间变量（偏导数）。 

在上述前向传播计算图中的单隐藏层简单网络的参数是 $W^{(1)}$和 $W^{(2)}$ 。 反向传播的目的是计算梯度$\partial J/ \partial W^{(1)}$和 $\partial J/ \partial W^{(2)}$ 。 为此，我们应用链式法则，依次计算每个中间变量和参数的梯度。 

 计算的顺序与前向传播中执行的顺序相反，因为我们需要从计算图的结果开始，并朝着参数的方向努力。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-243.png)

## 训练神经网络

 在训练神经网络时，前向传播和反向传播相互依赖。 对于前向传播，我们沿着依赖的方向遍历计算图并计算其路径上的所有变量。 然后将这些用于反向传播，其中计算顺序与计算图的相反。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-244.png)

 因此，在训练神经网络时，在初始化模型参数后， 我们交替使用前向传播和反向传播，利用反向传播给出的梯度来更新模型参数。

 注意，反向传播重复利用前向传播中存储的中间值，以避免重复计算。 带来的影响之一是我们需要保留中间值，直到反向传播完成。 这也是训练比单纯的预测需要更多的内存（显存）的原因之一。 此外，这些中间值的大小与网络层的数量和批量的大小大致成正比。 因此，使用更大的批量来训练更深层次的网络更容易导致`内存不足`（out of memory）错误。 



## 小结

- 前向传播在神经网络定义的计算图中按顺序计算和存储中间变量，它的顺序是从输入层到输出层。
- 反向传播按相反的顺序（从输出层到输入层）计算和存储神经网络的中间变量和参数的梯度。
- 在训练深度学习模型时，前向传播和反向传播是相互依赖的。
- 训练比预测需要更多的内存。

# 数值稳定性和模型初始化

## 梯度消失和梯度爆炸

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-245.png)

 不稳定梯度带来的风险不止在于数值表示； 不稳定梯度也威胁到我们优化算法的稳定性。 我们可能面临一些问题。

* 要么是`梯度爆炸`（gradient exploding）问题： 参数更新过大，破坏了模型的稳定收敛
* 要么是`梯度消失`（gradient vanishing）问题： 参数更新过小，在每次更新时几乎不会移动，导致模型无法学习。 

> 梯度消失

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-246.png)

 正如上图，当sigmoid函数的输入很大或是很小时，它的梯度都会消失。 此外，当反向传播通过许多层时，除非我们在刚刚好的地方， 这些地方sigmoid函数的输入接近于零，否则整个乘积的梯度可能会消失。 当我们的网络有很多层时，除非我们很小心，否则在某一层可能会切断梯度 

> 梯度爆炸

 为了更好地说明这一点，我们生成100个高斯随机矩阵，并将它们与某个初始矩阵相乘。 对于我们选择的尺度（方差 $\sigma ^ 2 =1 $），矩阵乘积发生爆炸。 当这种情况是由于深度网络的初始化所导致时，我们没有机会让梯度下降优化器收敛 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-247.png)

> 打破对称性

 神经网络设计中的另一个问题是其参数化所固有的对称性。 

假设我们有一个简单的多层感知机，它有一个隐藏层和两个隐藏单元。 在这种情况下，我们可以对第一层的权重$W^{(1)}$ 进行重排列， 并且同样对输出层的权重进行重排列，可以获得相同的函数。 第一个隐藏单元与第二个隐藏单元没有什么特别的区别。 换句话说，我们在每一层的隐藏单元之间具有排列对称性。 

 假设输出层将上述两个隐藏单元的多层感知机转换为仅一个输出单元。 想象一下，如果我们将隐藏层的所有参数初始化为$W^{(1)} =c$， $c$为常量，会发生什么？ 在这种情况下，在前向传播期间，两个隐藏单元采用相同的输入和参数， 产生相同的激活，该激活被送到输出单元。 

 在反向传播期间，根据参数 $W^{(1)}$ 对输出单元进行微分， 得到一个梯度，其元素都取相同的值。  

因此，在基于梯度的迭代（例如，小批量随机梯度下降）之后，  $W^{(1)}$ 的所有元素仍然采用相同的值。 这样的迭代永远不会打破对称性，我们可能永远也无法实现网络的表达能力。 隐藏层的行为就好像只有一个单元。 

 请注意，虽然小批量随机梯度下降不会打破这种对称性，但暂退法正则化可以。 

## 参数初始化

 解决（或至少减轻）上述问题的一种方法是进行参数初始化， 优化期间的注意和适当的正则化也可以进一步提高稳定性 

### 默认初始化

 在前面的部分中我们使用正态分布来初始化权重值。如果我们不指定初始化方法， 框架将使用默认的随机初始化方法，对于中等难度的问题，这种方法通常很有效。 

### Xavier初始化

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-248.png)

## 小结

- 梯度消失和梯度爆炸是深度网络中常见的问题。在参数初始化时需要非常小心，以确保梯度和参数可以得到很好的控制。
- 需要用启发式的初始化方法来确保初始梯度既不太大也不太小。
- ReLU激活函数缓解了梯度消失问题，这样可以加速收敛。
- 随机初始化是保证在进行优化前打破对称性的关键。
- Xavier初始化表明，对于每一层，输出的方差不受输入数量的影响，任何梯度的方差不受输出数量的影响



