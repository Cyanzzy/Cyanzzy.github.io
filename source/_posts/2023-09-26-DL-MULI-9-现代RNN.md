---
title: DL-MULI-9-现代RNN
date: 2023-09-26 09:33:44
tags: 
  - DL
categories: 
  - Science
---

# 门控循环单元 GRU

我们讨论了如何在循环神经网络中计算梯度， 以及矩阵连续乘积可以导致梯度消失或梯度爆炸的问题。 下面我们简单思考一下这种梯度异常在实践中的意义： 

- 我们可能会遇到这样的情况：早期观测值对预测所有未来观测值具有非常重要的意义。 考虑一个极端情况，其中第一个观测值包含一个校验和， 目标是在序列的末尾辨别校验和是否正确。 在这种情况下，第一个词元的影响至关重要。 我们希望有某些机制能够在一个记忆元里存储重要的早期信息。 如果没有这样的机制，我们将不得不给这个观测值指定一个非常大的梯度， 因为它会影响所有后续的观测值。
- 我们可能会遇到这样的情况：一些词元没有相关的观测值。 例如，在对网页内容进行情感分析时， 可能有一些辅助HTML代码与网页传达的情绪无关。 我们希望有一些机制来*跳过*隐状态表示中的此类词元。
- 我们可能会遇到这样的情况：序列的各个部分之间存在逻辑中断。 例如，书的章节之间可能会有过渡存在， 或者证券的熊市和牛市之间可能会有过渡存在。 在这种情况下，最好有一种方法来*重置*我们的内部状态表示。

## 门控隐状态

> 门控循环单元与普通的循环神经网络之间的关键区别在于： 

前者支持隐状态的门控。 这意味着模型有专门的机制来确定应该何时更新隐状态， 以及应该何时重置隐状态。 

 这些机制是可学习的，并且能够解决了上面列出的问题。 例如，如果第一个词元非常重要， 模型将学会在第一次观测之后不更新隐状态。 同样，模型也可以学会跳过不相关的临时观测。 最后，模型还将学会在需要的时候重置隐状态。 

### 重置门和更新门

 首先介绍`重置门`（reset gate）和`更新门`（update gate）。 我们把它们设计成 $(0, 1)$ 区间中的向量， 这样我们就可以进行凸组合。 重置门允许我们控制“可能还想记住”的过去状态的数量； 更新门将允许我们控制新状态中有多少个是旧状态的副本。 

 我们从构造这些门控开始。下图描述了门控循环单元中的重置门和更新门的输入， 输入是由当前时间步的输入和前一时间步的隐状态给出。 两个门的输出是由使用sigmoid激活函数的两个全连接层给出。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-288.png)

 对于给定的时间步 $t$， 假设输入是一个小批量 $\mathbf{X}_t \in \mathbb{R}^{n \times d}$ （样本个数 $n$ ，输入个数 $d$ ）  ， 上一个时间步的隐状态是 $\mathbf{H}_{t-1} \in \mathbb{R}^{n \times h}$（隐藏单元个数 $h$ ）。 那么，重置门 $\mathbf{R}_t \in \mathbb{R}^{n \times h}$ 和 更新门 $\mathbf {Z}_t \in \mathbb{R}^{n \times h}$ 的计算如下所示： 
$$
\begin{split}\begin{aligned} \mathbf{R}_t = \sigma(\mathbf{X}_t \mathbf{W}_{xr} + \mathbf{H}_{t-1} \mathbf{W}_{hr} + \mathbf{b}_r),\\ \mathbf{Z}_t = \sigma(\mathbf{X}_t \mathbf{W}_{xz} + \mathbf{H}_{t-1} \mathbf{W}_{hz} + \mathbf{b}_z), \end{aligned}\end{split}
$$
 其中 $\mathbf{W}_{xr}, \mathbf{W}_{xz} \in \mathbb{R}^{d \times h}$ 和 $\mathbf{W}_{hr}, \mathbf{W}_{hz} \in \mathbb{R}^{h \times h}$ 是权重参数，$\mathbf{b}_r, \mathbf{b}_z \in \mathbb{R}^{1 \times h}$ 是偏置参数。  

 请注意，在求和过程中会触发广播机制 。 我们使用sigmoid函数 将输入值转换到区间 $(0, 1)$ 

### 候选隐状态

 接下来，让我们将重置门 $\mathbf{R}_t$ 与   $(9.1.1)$ 常规隐状态更新机制集成， 得到在时间步 $t$ 的`候选隐状态`（candidate hidden state） $\tilde{\mathbf{H}}_t \in \mathbb{R}^{n \times h}$。 
$$
\mathbf{H}_t = \phi(\mathbf{X}_t \mathbf{W}_{xh} + \mathbf{H}_{t-1} \mathbf{W}_{hh}  + \mathbf{b}_h),\tag{9.1.1}
$$

$$
\tilde{\mathbf{H}}_t = \tanh(\mathbf{X}_t \mathbf{W}_{xh} + \left(\mathbf{R}_t \odot \mathbf{H}_{t-1}\right) \mathbf{W}_{hh} + \mathbf{b}_h),\tag{9.1.2}
$$



 其中 $\mathbf{W}_{xh} \in \mathbb{R}^{d \times h}$ 和 $\mathbf{W}_{hh} \in \mathbb{R}^{h \times h}$ 是权重参数， $\mathbf{b}_h \in \mathbb{R}^{1 \times h}$ 是偏置项， 符号 $\odot$ 是Hadamard积（按元素乘积）运算符。 在这里，我们使用tanh非线性激活函数来确保候选隐状态中的值保持在区间 $(−1,1)$ 中。 

 与  $(9.1.1)$  相比， $(9.1.2)$ 中的 $\mathbf{R}_t$ 和 $\mathbf{H}_{t-1}$ 的元素相乘可以减少以往状态的影响。 每当重置门 $\mathbf{R}_t$ 中的项接近1时， 我们恢复一个如 $(9.1.1)$  中的普通的循环神经网络。 对于重置门 $\mathbf{R}_t$ 中所有接近0的项， 候选隐状态是以 $\mathbf{X}_t$ 作为输入的多层感知机的结果。 因此，任何预先存在的隐状态都会被`重置` 为默认值。 

下图说明了应用重置门之后的计算流程。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-289.png)

### 隐状态

 上述的计算结果只是候选隐状态，我们仍然需要结合更新门 $\mathbf{Z}_t$ 的效果。 这一步确定新的隐状态 $\mathbf{H}_t \in \mathbb{R}^{n \times h}$ 在多大程度上来自旧的状态 $\mathbf{H}_{t-1}$ 和 新的候选状态 $\tilde{\mathbf{H}}_t$。 更新门 $\mathbf{Z}_t$ 仅需要在  $\mathbf{H}_{t-1}$ 和 $\tilde{\mathbf{H}}_t$ 之间进行按元素的凸组合就可以实现这个目标。 这就得出了门控循环单元的最终更新公式： 
$$
\mathbf{H}_t = \mathbf{Z}_t \odot \mathbf{H}_{t-1}  + (1 - \mathbf{Z}_t) \odot \tilde{\mathbf{H}}_t,\tag{9.1.3}
$$
 每当更新门 $\mathbf{Z}_t$ 接近 `1`时，模型就倾向只保留旧状态。 此时，来自 $\mathbf{X}_t$ 的信息基本上被忽略， 从而有效地跳过了依赖链条中的时间步 $t$。 相反，当 $\mathbf{Z}_t$ 接近 `0` 时， 新的隐状态 $\mathbf{H}_t$ 就会接近候选隐状态 $\tilde{\mathbf{H}}_t$ 。 

 这些设计可以帮助我们处理循环神经网络中的梯度消失问题， 并更好地捕获时间步距离很长的序列的依赖关系。 例如，如果整个子序列的所有时间步的更新门都接近于`1`， 则无论序列的长度如何，在序列起始时间步的旧隐状态都将很容易保留并传递到序列结束。 

 下图说明了更新门起作用后的计算流。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-290.png)

总之，门控循环单元具有以下两个显著特征：

- 重置门有助于捕获序列中的短期依赖关系；
- 更新门有助于捕获序列中的长期依赖关系。

## 从零开始实现

 为了更好地理解门控循环单元模型，我们从零开始实现它。 首先，我们读取时间机器数据集： 

```python
import torch
from torch import nn
from d2l import torch as d2l

batch_size, num_steps = 32, 35
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)
```

### 初始化模型参数

 下一步是初始化模型参数。 我们从标准差为0.01的高斯分布中提取权重， 并将偏置项设为0，超参数`num_hiddens`定义隐藏单元的数量， 实例化与更新门、重置门、候选隐状态和输出层相关的所有权重和偏置。 

```python
def get_params(vocab_size, num_hiddens, device):
    num_inputs = num_outputs = vocab_size

    def normal(shape):
        return torch.randn(size=shape, device=device)*0.01

    def three():
        return (normal((num_inputs, num_hiddens)),
                normal((num_hiddens, num_hiddens)),
                torch.zeros(num_hiddens, device=device))

    W_xz, W_hz, b_z = three()  # 更新门参数
    W_xr, W_hr, b_r = three()  # 重置门参数
    W_xh, W_hh, b_h = three()  # 候选隐状态参数
    # 输出层参数
    W_hq = normal((num_hiddens, num_outputs))
    b_q = torch.zeros(num_outputs, device=device)
    # 附加梯度
    params = [W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q]
    for param in params:
        param.requires_grad_(True)
    return params
```

### 定义模型

 现在我们将定义隐状态的初始化函数`init_gru_state`， 此函数返回一个形状为`（批量大小，隐藏单元个数）`的张量，张量的值全部为零。 

```python
def init_gru_state(batch_size, num_hiddens, device):
    return (torch.zeros((batch_size, num_hiddens), device=device), )
```

 现在我们准备定义门控循环单元模型， 模型的架构与基本的循环神经网络单元是相同的， 只是权重更新公式更为复杂。 

```python
def gru(inputs, state, params):
    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params
    H, = state
    outputs = []
    for X in inputs:
        Z = torch.sigmoid((X @ W_xz) + (H @ W_hz) + b_z)
        R = torch.sigmoid((X @ W_xr) + (H @ W_hr) + b_r)
        H_tilda = torch.tanh((X @ W_xh) + ((R * H) @ W_hh) + b_h)
        H = Z * H + (1 - Z) * H_tilda
        Y = H @ W_hq + b_q
        outputs.append(Y)
    return torch.cat(outputs, dim=0), (H,)
```

### 训练与预测

 训练结束后，我们分别打印输出训练集的困惑度， 以及前缀“time traveler”和“traveler”的预测序列上的困惑度。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-291.png)



## 简洁实现

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-292.png)

## 小结

- 门控循环神经网络可以更好地捕获时间步距离很长的序列上的依赖关系。
- 重置门有助于捕获序列中的短期依赖关系。
- 更新门有助于捕获序列中的长期依赖关系。
- 重置门打开时，门控循环单元包含基本循环神经网络；更新门打开时，门控循环单元可以跳过子序列。

# 长短期记忆网络 LSTM 

 长期以来，隐变量模型存在着长期信息保存和短期输入缺失的问题。 解决这一问题的最早方法之一是长短期存储器（long short-term memory，LSTM）。  

## 门控记忆元

 可以说，长短期记忆网络的设计灵感来自于计算机的逻辑门。 长短期记忆网络引入了`记忆元`（memory cell），或简称为`单元`（cell）。 有些文献认为记忆元是隐状态的一种特殊类型， 它们与隐状态具有相同的形状，其设计目的是用于记录附加的信息。  

 为了控制记忆元，我们需要许多门。 其中一个门用来从单元中输出条目，我们将其称为`输出门`（output gate）。 另外一个门用来决定何时将数据读入单元，我们将其称为`输入门`（input gate）。 我们还需要一种机制来重置单元的内容，由`遗忘门`（forget gate）来管理， 这种设计的动机与门控循环单元相同， 能够通过专用机制决定什么时候记忆或忽略隐状态中的输入。 

### 输入门、忘记门和输出门

 就如在门控循环单元中一样，`当前时间步的输入和前一个时间步的隐状态`作为数据送入长短期记忆网络的门中， 如下图所示。 它们由三个具有sigmoid激活函数的全连接层处理， 以计算输入门、遗忘门和输出门的值。 因此，这三个门的值都在 $(0,1)$ 的范围内。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-293.png)

 假设有 $h$ 个隐藏单元，批量大小为 $n$，输入数为 $d$。 因此，输入为 $\mathbf{X}_t \in \mathbb{R}^{n \times d}$ ， 前一时间步的隐状态为 $\mathbf{H}_{t-1} \in \mathbb{R}^{n \times h}$。 相应地，时间步 $t$ 的门被定义如下： 输入门是 $\mathbf{I}_t \in \mathbb{R}^{n \times h}$， 遗忘门是 $\mathbf{F}_t \in \mathbb{R}^{n \times h}$， 输出门是 $\mathbf{O}_t \in \mathbb{R}^{n \times h}$。 它们的计算方法如下： 
$$
\begin{split}\begin{aligned}
\mathbf{I}_t &= \sigma(\mathbf{X}_t \mathbf{W}_{xi} + \mathbf{H}_{t-1} \mathbf{W}_{hi} + \mathbf{b}_i),\\
\mathbf{F}_t &= \sigma(\mathbf{X}_t \mathbf{W}_{xf} + \mathbf{H}_{t-1} \mathbf{W}_{hf} + \mathbf{b}_f),\\
\mathbf{O}_t &= \sigma(\mathbf{X}_t \mathbf{W}_{xo} + \mathbf{H}_{t-1} \mathbf{W}_{ho} + \mathbf{b}_o),
\end{aligned}\end{split}\tag{9.2.1}
$$
 其中 $\mathbf{W}_{xi}, \mathbf{W}_{xf}, \mathbf{W}_{xo} \in \mathbb{R}^{d \times h}$ 和 $\mathbf{W}_{hi}, \mathbf{W}_{hf}, \mathbf{W}_{ho} \in \mathbb{R}^{h \times h}$ 是权重参数，$\mathbf{b}_i, \mathbf{b}_f, \mathbf{b}_o \in \mathbb{R}^{1 \times h}$ 是偏置参数。 

### 候选记忆元

 由于还没有指定各种门的操作，所以先介绍`候选记忆元`（candidate memory cell）$\tilde{\mathbf{C}}_t \in \mathbb{R}^{n \times h}$ 。 它的计算与上面描述的三个门的计算类似， 但是使用tanh函数作为激活函数，函数的值范围为 $(−1,1)$。 下面导出在时间步 $t$ 处的方程 
$$
\tilde{\mathbf{C}}_t = \text{tanh}(\mathbf{X}_t \mathbf{W}_{xc} + \mathbf{H}_{t-1} \mathbf{W}_{hc} + \mathbf{b}_c),\tag{9.2.2}
$$
 其中 $\mathbf{W}_{xc} \in \mathbb{R}^{d \times h}$ 和 $\mathbf{W}_{hc} \in \mathbb{R}^{h \times h}S$ 是权重参数， $\mathbf{b}_c \in \mathbb{R}^{1 \times h}$ 是偏置参数。  

 候选记忆元的如下图所示。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-294.png)

### 记忆元

 在门控循环单元中，有一种机制来控制输入和遗忘（或跳过）。 类似地，在长短期记忆网络中，也有两个门用于这样的目的： 输入门 $\mathbf{I}_t$ 控制采用多少来自 $\tilde{\mathbf{C}}_t$ 的新数据， 而遗忘门 $\mathbf{F}_t$ 控制保留多少过去的 记忆元 $\mathbf{C}_{t-1} \in \mathbb{R}^{n \times h}$ 的内容。 使用按元素乘法，得出： 
$$
\mathbf{C}_t = \mathbf{F}_t \odot \mathbf{C}_{t-1} + \mathbf{I}_t \odot \tilde{\mathbf{C}}_t,\tag{9.2.3}
$$
 如果遗忘门始终为`1`且输入门始终为`0`， 则过去的记忆元 $\mathbf{C}_{t-1}$ 将随时间被保存并传递到当前时间步。 引入这种设计是为了缓解梯度消失问题， 并更好地捕获序列中的长距离依赖关系。 

 这样我们就得到了计算记忆元的流程图：

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-295.png)

### 隐状态

 最后，我们需要定义如何计算隐状态 $\mathbf{H}_t \in \mathbb{R}^{n \times h}$， 这就是输出门发挥作用的地方。 在长短期记忆网络中，它仅仅是记忆元的tanh的门控版本。 这就确保了 $\mathbf{H}_t$ 的值始终在区间 $(−1,1)$ 内： 
$$
\mathbf{H}_t = \mathbf{O}_t \odot \tanh(\mathbf{C}_t),\tag{9.2.4}
$$
 只要输出门接近`1`，我们就能够有效地将所有记忆信息传递给预测部分， 而对于输出门接近`0`，我们只保留记忆元内的所有信息，而不需要更新隐状态 

下图提供了数据流的图形化演示：

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-296.png)

## 从零开始实现

 现在，我们从零开始实现长短期记忆网络， 首先加载时光机器数据集。 

```python
import torch
from torch import nn
from d2l import torch as d2l

batch_size, num_steps = 32, 35
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)
```

### 初始化模型参数

 接下来，我们需要定义和初始化模型参数。 如前所述，超参数`num_hiddens`定义隐藏单元的数量。 我们按照标准差0.01的高斯分布初始化权重，并将偏置项设为0。 

```python
def get_lstm_params(vocab_size, num_hiddens, device):
    num_inputs = num_outputs = vocab_size

    def normal(shape):
        return torch.randn(size=shape, device=device)*0.01

    def three():
        return (normal((num_inputs, num_hiddens)),
                normal((num_hiddens, num_hiddens)),
                torch.zeros(num_hiddens, device=device))

    W_xi, W_hi, b_i = three()  # 输入门参数
    W_xf, W_hf, b_f = three()  # 遗忘门参数
    W_xo, W_ho, b_o = three()  # 输出门参数
    W_xc, W_hc, b_c = three()  # 候选记忆元参数
    # 输出层参数
    W_hq = normal((num_hiddens, num_outputs))
    b_q = torch.zeros(num_outputs, device=device)
    # 附加梯度
    params = [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc,
              b_c, W_hq, b_q]
    for param in params:
        param.requires_grad_(True)
    return params
```

### 定义模型

 在初始化函数中， 长短期记忆网络的隐状态需要返回一个额外的记忆元， 单元的值为0，形状为`（批量大小，隐藏单元数）`。 因此，我们得到以下的状态初始化。 

```python
def init_lstm_state(batch_size, num_hiddens, device):
    return (torch.zeros((batch_size, num_hiddens), device=device),
            torch.zeros((batch_size, num_hiddens), device=device))
```

 实际模型的定义与我们前面讨论的一样： 提供三个门和一个额外的记忆元。 请注意，只有隐状态才会传递到输出层， 而记忆元 $\mathbf{C}_t$ 不直接参与输出计算。 

```python
def lstm(inputs, state, params):
    [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c,
     W_hq, b_q] = params
    (H, C) = state
    outputs = []
    for X in inputs:
        I = torch.sigmoid((X @ W_xi) + (H @ W_hi) + b_i)
        F = torch.sigmoid((X @ W_xf) + (H @ W_hf) + b_f)
        O = torch.sigmoid((X @ W_xo) + (H @ W_ho) + b_o)
        C_tilda = torch.tanh((X @ W_xc) + (H @ W_hc) + b_c)
        C = F * C + I * C_tilda
        H = O * torch.tanh(C)
        Y = (H @ W_hq) + b_q
        outputs.append(Y)
    return torch.cat(outputs, dim=0), (H, C)
```

### 训练和预测

 通过实例化`RNNModelScratch`类来训练一个长短期记忆网络 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-297.png)

## 简洁实现

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-298.png)

 长短期记忆网络是典型的具有重要状态控制的隐变量自回归模型。 多年来已经提出了其许多变体，例如，多层、残差连接、不同类型的正则化。 然而，由于序列的长距离依赖性，训练长短期记忆网络 和其他序列模型（例如门控循环单元）的成本是相当高的 

## 小结

- 长短期记忆网络有三种类型的门：输入门、遗忘门和输出门。
- 长短期记忆网络的隐藏层输出包括“隐状态”和“记忆元”。只有隐状态会传递到输出层，而记忆元完全属于内部信息。
- 长短期记忆网络可以缓解梯度消失和梯度爆炸。

# 深度循环神经网络

 下图描述了一个具有 $L$ 个隐藏层的深度循环神经网络， 每个隐状态都连续地传递到当前层的下一个时间步和下一层的当前时间步 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-299.png)

## 函数依赖关系

 假设在时间步 $t$ 有一个小批量的输入数据 $\mathbf{X}_t \in \mathbb{R}^{n \times d}$（样本数：$n$，每个样本中的输入数：$d$）。 同时，将 $l^\mathrm{th}$ 隐藏层（$l=1,\ldots,L$） 的隐状态设为 $\mathbf{H}_t^{(l)} \in \mathbb{R}^{n \times h}$ （隐藏单元数：$h$）， 输出层变量设为 $\mathbf{O}_t \in \mathbb{R}^{n \times q}$（输出数：$q$）。 设置 $\mathbf{H}_t^{(0)} = \mathbf{X}_t$， 第 $l$ 个隐藏层的隐状态使用激活函数 $\phi_l$，则： 
$$
\mathbf{H}_t^{(l)} = \phi_l(\mathbf{H}_t^{(l-1)} \mathbf{W}_{xh}^{(l)} + \mathbf{H}_{t-1}^{(l)} \mathbf{W}_{hh}^{(l)}  + \mathbf{b}_h^{(l)}),\tag{9.3.1}
$$
其中，权重 $\mathbf{W}_{xh}^{(l)} \in \mathbb{R}^{h \times h}$， $\mathbf{W}_{hh}^{(l)} \in \mathbb{R}^{h \times h}$ 和 偏置 $\mathbf{b}_h^{(l)} \in \mathbb{R}^{1 \times h}$ 都是第 $l$ 个隐藏层的模型参数。

最后，输出层的计算仅基于第 $l$ 个隐藏层最终的隐状态：
$$
\mathbf{O}_t = \mathbf{H}_t^{(L)} \mathbf{W}_{hq} + \mathbf{b}_q,\tag{9.3.2}
$$
 其中，权重 $\mathbf{W}_{hq} \in \mathbb{R}^{h \times q}$ 和偏置 $\mathbf{b}_q \in \mathbb{R}^{1 \times q}$ 都是输出层的模型参数。 

 与多层感知机一样，隐藏层数目 $L$ 和隐藏单元数目 $h$ 都是超参数。 也就是说，它们可以由我们调整的。

 另外，用门控循环单元或长短期记忆网络的隐状态 来代替 $(9.3.1)$ 中的隐状态进行计算， 可以很容易地得到深度门控循环神经网络或深度长短期记忆神经网络。 

## 简洁实现

 以长短期记忆网络模型为例， 我们指定了层的数量， 而不是使用单一层这个默认值。 像往常一样，我们从加载数据集开始。 

```python
import torch
from torch import nn
from d2l import torch as d2l

batch_size, num_steps = 32, 35
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)
```



 像选择超参数这类架构决策也跟 LSTM 中的决策非常相似。 因为我们有不同的词元，所以输入和输出都选择相同数量，即`vocab_size`。 隐藏单元的数量仍然是256。 唯一的区别是，我们现在通过`num_layers`的值来设定隐藏层数。 

```python
vocab_size, num_hiddens, num_layers = len(vocab), 256, 2
num_inputs = vocab_size
device = d2l.try_gpu()
lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers)
model = d2l.RNNModel(lstm_layer, len(vocab))
model = model.to(device)
```

## 训练与预测

由于使用了长短期记忆网络模型来实例化两个层，因此训练速度被大大降低了。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-300.png)

## 小结

- 在深度循环神经网络中，隐状态的信息被传递到当前层的下一时间步和下一层的当前时间步。
- 有许多不同风格的深度循环神经网络， 如长短期记忆网络、门控循环单元、或经典循环神经网络。 这些模型在深度学习框架的高级API中都有涵盖。
- 总体而言，深度循环神经网络需要大量的调参（如学习率和修剪） 来确保合适的收敛，模型的初始化也需要谨慎。

# 双向循环神经网络

在序列学习中，我们以往假设的目标是： 在给定观测的情况下 （例如，在时间序列的上下文中或在语言模型的上下文中）， 对下一个输出进行建模。 虽然这是一个典型情景，但不是唯一的。 还可能发生什么其它的情况呢？ 我们考虑以下三个在文本序列中填空的任务。

- 我`___`。
- 我`___`饿了。
- 我`___`饿了，我可以吃半头猪。

根据可获得的信息量，我们可以用不同的词填空， 如“很高兴”（“happy”）、“不”（“not”）和“非常”（“very”）。 很明显，每个短语的`下文`传达了重要信息（如果有的话）， 而这些信息关乎到选择哪个词来填空， 所以无法利用这一点的序列模型将在相关任务上表现不佳。 

例如，如果要做好命名实体识别 （例如，识别“Green”指的是“格林先生”还是绿色）， 不同长度的上下文范围重要性是相同的。 为了获得一些解决问题的灵感，让我们先迂回到概率图模型。

## 隐马尔可夫模型中的动态规划

 如果我们想用概率图模型来解决这个问题， 可以设计一个隐变量模型： 在任意时间步 $t$，假设存在某个隐变量 $h_t$， 通过概率 $P(x_t \mid h_t)$ 控制我们观测到的 $x_t$。 此外，任何 $h_t \to h_{t+1}$ 转移 都是由一些状态转移概率 $P(h_{t+1} \mid h_{t})$ 给出。 这个概率图模型就是一个`隐马尔可夫模型`（hidden Markov model，HMM） 

​	![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-301.png)

 因此，对于有 $T$ 个观测值的序列， 我们在观测状态和隐状态上具有以下联合概率分布： 
$$
P(x_1, \ldots, x_T, h_1, \ldots, h_T) = \prod_{t=1}^T P(h_t \mid h_{t-1}) P(x_t \mid h_t), \text{ where } P(h_1 \mid h_0) = P(h_1).\tag{9.4.1}
$$
现在，假设我们观测到所有的 $x_i$，除了 $x_j$， 并且我们的目标是计算 $P(x_j \mid x_{-j})$， 其中 $x_{-j} = (x_1, \ldots, x_{j-1}, x_{j+1}, \ldots, x_{T})$。 由于 $P(x_j \mid x_{-j})$ 中没有隐变量， 因此我们考虑对 $h_1, \ldots, h_T$ 选择构成的 所有可能的组合进行求和。 如果任何 $h_i$ 可以接受 $k$ 个不同的值（有限的状态数）， 这意味着我们需要对 $k^T$ 个项求和， 这个任务显然难于登天。 幸运的是，有个巧妙的解决方案：`动态规划`（dynamic programming）。

要了解动态规划的工作方式， 我们考虑对隐变量 $h_1, \ldots, h_T$ 的依次求和。 根据 $(9.4.1)$，将得出：
$$
\begin{split}\begin{aligned}
    &P(x_1, \ldots, x_T) \\
    =& \sum_{h_1, \ldots, h_T} P(x_1, \ldots, x_T, h_1, \ldots, h_T) \\
    =& \sum_{h_1, \ldots, h_T} \prod_{t=1}^T P(h_t \mid h_{t-1}) P(x_t \mid h_t) \\
    =& \sum_{h_2, \ldots, h_T} \underbrace{\left[\sum_{h_1} P(h_1) P(x_1 \mid h_1) P(h_2 \mid h_1)\right]}_{\pi_2(h_2) \stackrel{\mathrm{def}}{=}}
    P(x_2 \mid h_2) \prod_{t=3}^T P(h_t \mid h_{t-1}) P(x_t \mid h_t) \\
    =& \sum_{h_3, \ldots, h_T} \underbrace{\left[\sum_{h_2} \pi_2(h_2) P(x_2 \mid h_2) P(h_3 \mid h_2)\right]}_{\pi_3(h_3)\stackrel{\mathrm{def}}{=}}
    P(x_3 \mid h_3) \prod_{t=4}^T P(h_t \mid h_{t-1}) P(x_t \mid h_t)\\
    =& \dots \\
    =& \sum_{h_T} \pi_T(h_T) P(x_T \mid h_T).
\end{aligned}\end{split},\tag{9.4.2}
$$
 通常，我们将`前向递归`（forward recursion）写为： 
$$
\pi_{t+1}(h_{t+1}) = \sum_{h_t} \pi_t(h_t) P(x_t \mid h_t) P(h_{t+1} \mid h_t).\tag{9.4.3}
$$
递归被初始化为 $\pi_1(h_1) = P(h_1)$ 。 符号简化，也可以写成 $\pi_{t+1} = f(\pi_t, x_t)$ ， 其中 $f$ 是一些可学习的函数。 这看起来就像我们在循环神经网络中讨论的隐变量模型中的更新方程。

与前向递归一样，我们也可以使用后向递归对同一组隐变量求和。这将得到：
$$
\begin{split}\begin{aligned}
    & P(x_1, \ldots, x_T) \\
     =& \sum_{h_1, \ldots, h_T} P(x_1, \ldots, x_T, h_1, \ldots, h_T) \\
    =& \sum_{h_1, \ldots, h_T} \prod_{t=1}^{T-1} P(h_t \mid h_{t-1}) P(x_t \mid h_t) \cdot P(h_T \mid h_{T-1}) P(x_T \mid h_T) \\
    =& \sum_{h_1, \ldots, h_{T-1}} \prod_{t=1}^{T-1} P(h_t \mid h_{t-1}) P(x_t \mid h_t) \cdot
    \underbrace{\left[\sum_{h_T} P(h_T \mid h_{T-1}) P(x_T \mid h_T)\right]}_{\rho_{T-1}(h_{T-1})\stackrel{\mathrm{def}}{=}} \\
    =& \sum_{h_1, \ldots, h_{T-2}} \prod_{t=1}^{T-2} P(h_t \mid h_{t-1}) P(x_t \mid h_t) \cdot
    \underbrace{\left[\sum_{h_{T-1}} P(h_{T-1} \mid h_{T-2}) P(x_{T-1} \mid h_{T-1}) \rho_{T-1}(h_{T-1}) \right]}_{\rho_{T-2}(h_{T-2})\stackrel{\mathrm{def}}{=}} \\
    =& \ldots \\
    =& \sum_{h_1} P(h_1) P(x_1 \mid h_1)\rho_{1}(h_{1}).
\end{aligned}\end{split},\tag{9.4.4}
$$
 因此，我们可以将`后向递归`（backward recursion）写为： 
$$
\rho_{t-1}(h_{t-1})= \sum_{h_{t}} P(h_{t} \mid h_{t-1}) P(x_{t} \mid h_{t}) \rho_{t}(h_{t}),\tag{9.4.5}
$$
 初始化 $\rho_T(h_T) = 1$。 前向和后向递归都允许我们对 $T$ 个隐变量在 $\mathcal{O}(kT)$ （线性而不是指数）时间内对 $(h_1, \ldots, h_T)$ 的所有值求和。 这是使用图模型进行概率推理的巨大好处之一。 它也是通用消息传递算法 ([Aji and McEliece, 2000](https://zh-v2.d2l.ai/chapter_references/zreferences.html#id4))的一个非常特殊的例子。 结合前向和后向递归，我们能够计算 
$$
P(x_j \mid x_{-j}) \propto \sum_{h_j} \pi_j(h_j) \rho_j(h_j) P(x_j \mid h_j).\tag{9.4.6}
$$
 因为符号简化的需要，后向递归也可以写为 $\rho_{t-1} = g(\rho_t, x_t)$， 其中 $g$ 是一个可以学习的函数。 同样，这看起来非常像一个更新方程， 只是不像我们在循环神经网络中看到的那样前向运算，而是后向计算。 事实上，知道未来数据何时可用对隐马尔可夫模型是有益的。 信号处理学家将是否知道未来观测这两种情况区分为内插和外推 

## 双向模型

 如果我们希望在循环神经网络中拥有一种机制， 使之能够提供与隐马尔可夫模型类似的前瞻能力， 我们就需要修改循环神经网络的设计。

 幸运的是，这在概念上很容易， 只需要增加一个`从最后一个词元开始从后向前运行`的循环神经网络， 而不是只有一个在前向模式下`从第一个词元开始运行`的循环神经网络。  `双向循环神经网络`（bidirectional RNNs） 添加了反向传递信息的隐藏层，以便更灵活地处理此类信息。

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-302.png)

 事实上，这与隐马尔可夫模型中的动态规划的前向和后向递归没有太大区别。 其主要区别是，在隐马尔可夫模型中的方程具有特定的统计意义。 

双向循环神经网络没有这样容易理解的解释， 我们只能把它们当作通用的、可学习的函数。 这种转变集中体现了现代深度网络的设计原则： 首先使用经典统计模型的函数依赖类型，然后将其参数化为通用形式。 

### 定义

 双向循环神经网络是由 ([Schuster and Paliwal, 1997](https://zh-v2.d2l.ai/chapter_references/zreferences.html#id146))提出的， 关于各种架构的详细讨论请参阅 ([Graves and Schmidhuber, 2005](https://zh-v2.d2l.ai/chapter_references/zreferences.html#id51))。 让我们看看这样一个网络的细节。 

 对于任意时间步 $t$，给定一个小批量的输入数据 $\mathbf{X}_t \in \mathbb{R}^{n \times d}$（样本数 $n$，每个示例中的输入数 $d$）， 并且令隐藏层激活函数为 $\phi$。 在双向架构中，我们设该时间步的前向和反向隐状态分别为 $\overrightarrow{\mathbf{H}}_t \in \mathbb{R}^{n \times h}$ 和 $\overleftarrow{\mathbf{H}}_t \in \mathbb{R}^{n \times h}$， 其中 $h$ 是隐藏单元的数目。 前向和反向隐状态的更新如下： 
$$
\begin{split}\begin{aligned}
\overrightarrow{\mathbf{H}}_t &= \phi(\mathbf{X}_t \mathbf{W}_{xh}^{(f)} + \overrightarrow{\mathbf{H}}_{t-1} \mathbf{W}_{hh}^{(f)}  + \mathbf{b}_h^{(f)}),\\
\overleftarrow{\mathbf{H}}_t &= \phi(\mathbf{X}_t \mathbf{W}_{xh}^{(b)} + \overleftarrow{\mathbf{H}}_{t+1} \mathbf{W}_{hh}^{(b)}  + \mathbf{b}_h^{(b)}),
\end{aligned}\end{split}\quad\tag{9.4.7}
$$
其中，权重 $\mathbf{W}_{xh}^{(f)} \in \mathbb{R}^{d \times h}, \mathbf{W}_{hh}^{(f)} \in \mathbb{R}^{h \times h}, \mathbf{W}_{xh}^{(b)} \in \mathbb{R}^{d \times h}, \mathbf{W}_{hh}^{(b)} \in \mathbb{R}^{h \times h}$ 和偏置 $\mathbf{b}_h^{(f)} \in \mathbb{R}^{1 \times h}, \mathbf{b}_h^{(b)} \in \mathbb{R}^{1 \times h}$ 都是模型参数。

接下来，将前向隐状态 $\overrightarrow{\mathbf{H}}_t$ 和反向隐状态 $\overleftarrow{\mathbf{H}}_t$ 连接起来， 获得需要送入输出层的隐状态 $\mathbf{H}_t \in \mathbb{R}^{n \times 2h}$。 在具有多个隐藏层的深度双向循环神经网络中， 该信息作为输入传递到下一个双向层。 最后，输出层计算得到的输出为 $\mathbf{O}_t \in \mathbb{R}^{n \times q}$（$q$是输出单元的数目）：
$$
\mathbf{O}_t = \mathbf{H}_t \mathbf{W}_{hq} + \mathbf{b}_q.\quad\tag{9.4.8}
$$
 这里，权重矩阵 $\mathbf{W}_{hq} \in \mathbb{R}^{2h \times q}$ 和偏置 $\mathbf{b}_q \in \mathbb{R}^{1 \times q}$ 是输出层的模型参数。 事实上，这两个方向可以拥有不同数量的隐藏单元。 

### 模型的计算代价及其应用

 双向循环神经网络的一个关键特性是使用来自序列两端的信息来估计输出。 我们使用来自过去和未来的观测信息来预测当前的观测。 

但是在对下一个词元进行预测的情况中，这样的模型并不是我们所需的。 因为在预测下一个词元时，我们终究无法知道下一个词元的下文是什么， 所以将不会得到很好的精度。具体地说，在训练期间，我们能够利用过去和未来的数据来估计现在空缺的词； 而在测试期间，我们只有过去的数据，因此精度将会很差。 

 双向层的使用在实践中非常少，并且仅仅应用于部分场合。 例如，填充缺失的单词、词元注释（例如，用于命名实体识别） 以及作为序列处理流水线中的一个步骤对序列进行编码（例如，用于机器翻译）。

 另一个严重问题是，双向循环神经网络的计算速度非常慢。 其主要原因是网络的前向传播需要在双向层中进行前向和后向递归， 并且网络的反向传播还依赖于前向传播的结果。 因此，梯度求解将有一个非常长的链。 

## 小结

- 在双向循环神经网络中，每个时间步的隐状态由当前时间步的前后数据同时决定。
- 双向循环神经网络与概率图模型中的“前向-后向”算法具有相似性。
- 双向循环神经网络主要用于序列编码和给定双向上下文的观测估计。
- 由于梯度链更长，因此双向循环神经网络的训练代价非常高。

# 编码器-解码器架构

 机器翻译是序列转换模型的一个核心问题， 其输入和输出都是长度可变的序列。 

为了处理这种类型的输入和输出， 我们可以设计一个包含两个主要组件的架构： 

* 第一个组件是一个`编码器`（encoder）： 它接受一个长度可变的序列作为输入， 并将其转换为具有固定形状的编码状态。 
* 第二个组件是`解码器`（decoder）： 它将固定形状的编码状态映射到长度可变的序列。

这被称为`编码器-解码器`（encoder-decoder）架构 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-303.png)

 我们以英语到法语的机器翻译为例： 给定一个英文的输入序列：“They”“are”“watching”“.”。 

首先，这种“编码器－解码器”架构将长度可变的输入序列编码成一个“状态”， 然后对该状态进行解码， 一个词元接着一个词元地生成翻译后的序列作为输出： “Ils”“regordent”“.”。

## 编码器

 在编码器接口中，我们只指定长度可变的序列作为编码器的输入`X`。 任何继承这个`Encoder`基类的模型将完成代码实现。 

```python
from torch import nn


#@save
class Encoder(nn.Module):
    """编码器-解码器架构的基本编码器接口"""
    def __init__(self, **kwargs):
        super(Encoder, self).__init__(**kwargs)

    def forward(self, X, *args):
        raise NotImplementedError
```

## 解码器

 在下面的解码器接口中，我们新增一个`init_state`函数， 用于将编码器的输出（`enc_outputs`）转换为编码后的状态。  为了逐个地生成长度可变的词元序列， 解码器在每个时间步都会将输入 （例如：在前一时间步生成的词元）和编码后的状态 映射成当前时间步的输出词元。 

```python
#@save
class Decoder(nn.Module):
    """编码器-解码器架构的基本解码器接口"""
    def __init__(self, **kwargs):
        super(Decoder, self).__init__(**kwargs)

    def init_state(self, enc_outputs, *args):
        raise NotImplementedError

    def forward(self, X, state):
        raise NotImplementedError
```

## 合并编码器和解码器

 总而言之，“编码器-解码器”架构包含了一个编码器和一个解码器， 并且还拥有可选的额外的参数。 在前向传播中，编码器的输出用于生成编码状态， 这个状态又被解码器作为其输入的一部分。 

```python
#@save
class EncoderDecoder(nn.Module):
    """编码器-解码器架构的基类"""
    def __init__(self, encoder, decoder, **kwargs):
        super(EncoderDecoder, self).__init__(**kwargs)
        self.encoder = encoder
        self.decoder = decoder

    def forward(self, enc_X, dec_X, *args):
        enc_outputs = self.encoder(enc_X, *args)
        dec_state = self.decoder.init_state(enc_outputs, *args)
        return self.decoder(dec_X, dec_state)
```

##  小结

- “编码器－解码器”架构可以将长度可变的序列作为输入和输出，因此适用于机器翻译等序列转换问题。
- 编码器将长度可变的序列作为输入，并将其转换为具有固定形状的编码状态。
- 解码器将具有固定形状的编码状态映射为长度可变的序列。

# 序列到序列学习 seq2seq

 遵循编码器－解码器架构的设计原则， 循环神经网络编码器使用长度可变的序列作为输入， 将其转换为固定形状的隐状态，即 输入序列的信息被*编码*到循环神经网络编码器的隐状态中。 

 为了连续生成输出序列的词元， 独立的循环神经网络解码器是基于输入序列的编码信息 和输出序列已经看见的或者生成的词元来预测下一个词元 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-304.png)

 在上图中， 特定的`<eos>`表示序列结束词元。 一旦输出序列生成此词元，模型就会停止预测。 

在循环神经网络解码器的初始化时间步，有两个特定的设计决定： 

* 首先，特定的`<bos>`表示序列开始词元，它是解码器的输入序列的第一个词元。
* 其次，使用循环神经网络编码器最终的隐状态来初始化解码器的隐状态。 例如，在 ([Sutskever *et al.*, 2014](https://zh-v2.d2l.ai/chapter_references/zreferences.html#id160))的设计中， 正是基于这种设计将输入序列的编码信息送入到解码器中来生成输出序列的。 在其他一些设计中 ([Cho *et al.*, 2014](https://zh-v2.d2l.ai/chapter_references/zreferences.html#id24))， 如上图所示， 编码器最终的隐状态在每一个时间步都作为解码器的输入序列的一部分。 

```python
import collections
import math
import torch
from torch import nn
from d2l import torch as d2l
```

## 编码器

 从技术上讲，编码器将长度可变的输入序列转换成 形状固定的上下文变量 $\mathbf{c}$， 并且将输入序列的信息在该上下文变量中进行编码。 如上述图所示，可以使用循环神经网络来设计编码器。 

 考虑由一个序列组成的样本（批量大小是1）。 假设输入序列是$x_1, \ldots, x_T$， 其中 $x_t$ 是输入文本序列中的第 $t$ 个词元。 在时间步 $t$，循环神经网络将词元 $x_t$ 的输入特征向量 $\mathbf{x}_t$ 和 $\mathbf{h} _{t-1}$（即上一时间步的隐状态） 转换为 $\mathbf{h}_t$（即当前步的隐状态）。 使用一个函数 $f$ 来描述循环神经网络的循环层所做的变换： 
$$
\mathbf{h}_t = f(\mathbf{x}_t, \mathbf{h}_{t-1}).\tag{9.6.1}
$$
 总之，编码器通过选定的函数 $q$， 将所有时间步的隐状态转换为上下文变量： 
$$
\mathbf{c} =  q(\mathbf{h}_1, \ldots, \mathbf{h}_T).\tag{9.6.2}
$$
 比如，当选择 $q(\mathbf{h}_1, \ldots, \mathbf{h}_T) = \mathbf{h}_T$ 时 （就像上图中一样）， 上下文变量仅仅是输入序列在最后时间步的隐状态 $\mathbf{h}_T$。 

 到目前为止，我们使用的是一个单向循环神经网络来设计编码器， 其中隐状态只依赖于输入子序列， 这个子序列是由输入序列的开始位置到隐状态所在的时间步的位置 （包括隐状态所在的时间步）组成。 

我们也可以使用双向循环神经网络构造编码器， 其中隐状态依赖于两个输入子序列， 两个子序列是由隐状态所在的时间步的位置之前的序列和之后的序列 （包括隐状态所在的时间步）， 因此隐状态对整个序列的信息都进行了编码。 

 现在，让我们实现循环神经网络编码器。 注意，我们使用了`嵌入层`（embedding layer） 来获得输入序列中每个词元的特征向量。 嵌入层的权重是一个矩阵， 其行数等于输入词表的大小（`vocab_size`）， 其列数等于特征向量的维度（`embed_size`）。 对于任意输入词元的索引 $i$， 嵌入层获取权重矩阵的第 $i$ 行（从0开始）以返回其特征向量。 另外，本文选择了一个多层门控循环单元来实现编码器。 

```python
#@save
class Seq2SeqEncoder(d2l.Encoder):
    """用于序列到序列学习的循环神经网络编码器"""
    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
                 dropout=0, **kwargs):
        super(Seq2SeqEncoder, self).__init__(**kwargs)
        # 嵌入层
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers,
                          dropout=dropout)

    def forward(self, X, *args):
        # 输出'X'的形状：(batch_size,num_steps,embed_size)
        X = self.embedding(X)
        # 在循环神经网络模型中，第一个轴对应于时间步
        X = X.permute(1, 0, 2)
        # 如果未提及状态，则默认为0
        output, state = self.rnn(X)
        # output的形状:(num_steps,batch_size,num_hiddens)
        # state的形状:(num_layers,batch_size,num_hiddens)
        return output, state
```

 下面，我们实例化上述编码器的实现： 我们使用一个两层门控循环单元编码器，其隐藏单元数为16。 给定一小批量的输入序列`X` `（批量大小为4，时间步为7）`。 在完成所有时间步后， 最后一层的隐状态的输出是一个张量（`output`由编码器的循环层返回）， 其形状为`（时间步数，批量大小，隐藏单元数）`。 

```python
encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16,
                         num_layers=2)
encoder.eval()
X = torch.zeros((4, 7), dtype=torch.long)
output, state = encoder(X)
output.shape
```

 由于这里使用的是门控循环单元， 所以在最后一个时间步的多层隐状态的形状是 `（隐藏层的数量，批量大小，隐藏单元的数量）`。 如果使用长短期记忆网络，`state`中还将包含记忆单元信息。 

```python
state.shape
```

## 解码器

正如上文提到的，编码器输出的上下文变量 $\mathbf{c}$ 对整个输入序列 $x_1, \ldots, x_T$ 进行编码。 来自训练数据集的输出序列 $y_1, y_2, \ldots, y_{T'}$， 对于每个时间步 $t'$（与输入序列或编码器的时间步 $t$ 不同）， 解码器输出 $y_{t'}$ 的概率取决于先前的输出子序列  $y_1, \ldots, y_{t'-1}$ 和上下文变量 $\mathbf{c}$， 即 $P(y_{t'} \mid y_1, \ldots, y_{t'-1}, \mathbf{c})$。

为了在序列上模型化这种条件概率， 我们可以使用另一个循环神经网络作为解码器。 在输出序列上的任意时间步 $t^\prime$， 循环神经网络将来自上一时间步的输出 $y_{t^\prime-1}$ 和上下文变量 $\mathbf{c}$ 作为其输入， 然后在当前时间步将它们和上一隐状态 $\mathbf{s}_{t^\prime-1}$ 转换为 隐状态 $\mathbf{s}_{t^\prime}$。 因此，可以使用函数 $g$ 来表示解码器的隐藏层的变换：
$$
\mathbf{s}_{t^\prime} = g(y_{t^\prime-1}, \mathbf{c}, \mathbf{s}_{t^\prime-1}).\tag{9.6.3}
$$
 在获得解码器的隐状态之后， 我们可以使用输出层和softmax操作 来计算在时间步 $t^\prime$ 时输出 $y_{t^\prime}$ 的条件概率分布 $P(y_{t^\prime} \mid y_1, \ldots, y_{t^\prime-1}, \mathbf{c})$。 

 根据上述图，当实现解码器时， 我们直接使用编码器最后一个时间步的隐状态来初始化解码器的隐状态。 这就要求使用循环神经网络实现的编码器和解码器具有相同数量的层和隐藏单元。 

为了进一步包含经过编码的输入序列的信息， 上下文变量在所有的时间步与解码器的输入进行拼接（concatenate）。 为了预测输出词元的概率分布， 在循环神经网络解码器的最后一层使用全连接层来变换隐状态。 

```python
class Seq2SeqDecoder(d2l.Decoder):
    """用于序列到序列学习的循环神经网络解码器"""
    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
                 dropout=0, **kwargs):
        super(Seq2SeqDecoder, self).__init__(**kwargs)
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers,
                          dropout=dropout)
        self.dense = nn.Linear(num_hiddens, vocab_size)

    def init_state(self, enc_outputs, *args):
        return enc_outputs[1]

    def forward(self, X, state):
        # 输出'X'的形状：(batch_size,num_steps,embed_size)
        X = self.embedding(X).permute(1, 0, 2)
        # 广播context，使其具有与X相同的num_steps
        context = state[-1].repeat(X.shape[0], 1, 1)
        X_and_context = torch.cat((X, context), 2)
        output, state = self.rnn(X_and_context, state)
        output = self.dense(output).permute(1, 0, 2)
        # output的形状:(batch_size,num_steps,vocab_size)
        # state的形状:(num_layers,batch_size,num_hiddens)
        return output, state
```

 下面，我们用与前面提到的编码器中相同的超参数来实例化解码器。 如我们所见，解码器的输出形状变为`（批量大小，时间步数，词表大小）`， 其中张量的最后一个维度存储预测的词元分布。 

```python
decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8, num_hiddens=16,
                         num_layers=2)
decoder.eval()
state = decoder.init_state(encoder(X))
output, state = decoder(X, state)
output.shape, state.shape
```

 总之，上述循环神经网络“编码器－解码器”模型中的各层如下所示

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-305.png)

## 损失函数

 在每个时间步，解码器预测了输出词元的概率分布。 类似于语言模型，可以使用softmax来获得分布， 并通过计算交叉熵损失函数来进行优化。将特定的填充词元被添加到序列的末尾， 不同长度的序列可以以相同形状的小批量加载。 但是，我们应该将填充词元的预测排除在损失函数的计算之外。 

 我们使用下面的`sequence_mask`函数 通过零值化屏蔽不相关的项， 以便后面任何不相关预测的计算都是与零的乘积，结果都等于零。 例如，如果两个序列的有效长度（不包括填充词元）分别为1和2， 则第一个序列的第一项和第二个序列的前两项之后的剩余项将被清除为零。 

```python
#@save
def sequence_mask(X, valid_len, value=0):
    """在序列中屏蔽不相关的项"""
    maxlen = X.size(1)
    mask = torch.arange((maxlen), dtype=torch.float32,
                        device=X.device)[None, :] < valid_len[:, None]
    X[~mask] = value
    return X

X = torch.tensor([[1, 2, 3], [4, 5, 6]])
sequence_mask(X, torch.tensor([1, 2]))
```

 我们还可以使用此函数屏蔽最后几个轴上的所有项。如果愿意，也可以使用指定的非零值来替换这些项。 

```python
X = torch.ones(2, 3, 4)
sequence_mask(X, torch.tensor([1, 2]), value=-1)
```

 现在，我们可以通过扩展softmax交叉熵损失函数来遮蔽不相关的预测。 最初，所有预测词元的掩码都设置为1。 一旦给定了有效长度，与填充词元对应的掩码将被设置为0。 最后，将所有词元的损失乘以掩码，以过滤掉损失中填充词元产生的不相关预测。 

```python
#@save
class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):
    """带遮蔽的softmax交叉熵损失函数"""
    # pred的形状：(batch_size,num_steps,vocab_size)
    # label的形状：(batch_size,num_steps)
    # valid_len的形状：(batch_size,)
    def forward(self, pred, label, valid_len):
        weights = torch.ones_like(label)
        weights = sequence_mask(weights, valid_len)
        self.reduction='none'
        unweighted_loss = super(MaskedSoftmaxCELoss, self).forward(
            pred.permute(0, 2, 1), label)
        weighted_loss = (unweighted_loss * weights).mean(dim=1)
        return weighted_loss
```

 我们可以创建三个相同的序列来进行代码健全性检查， 然后分别指定这些序列的有效长度为4、2和0。 结果就是，第一个序列的损失应为第二个序列的两倍，而第三个序列的损失应为零。 

```python
loss = MaskedSoftmaxCELoss()
loss(torch.ones(3, 4, 10), torch.ones((3, 4), dtype=torch.long),
     torch.tensor([4, 2, 0]))
```

## 训练

 在下面的循环训练过程中，如最开始的图所示， 特定的序列开始词元（`<bos>`）和 原始的输出序列（不包括序列结束词元`<eos>`） 拼接在一起作为解码器的输入。 这被称为`强制教学`（teacher forcing）， 因为原始的输出序列（词元的标签）被送入解码器。 或者，将来自上一个时间步的`预测`得到的词元作为解码器的当前输入。 

```python
#@save
def train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device):
    """训练序列到序列模型"""
    def xavier_init_weights(m):
        if type(m) == nn.Linear:
            nn.init.xavier_uniform_(m.weight)
        if type(m) == nn.GRU:
            for param in m._flat_weights_names:
                if "weight" in param:
                    nn.init.xavier_uniform_(m._parameters[param])

    net.apply(xavier_init_weights)
    net.to(device)
    optimizer = torch.optim.Adam(net.parameters(), lr=lr)
    loss = MaskedSoftmaxCELoss()
    net.train()
    animator = d2l.Animator(xlabel='epoch', ylabel='loss',
                     xlim=[10, num_epochs])
    for epoch in range(num_epochs):
        timer = d2l.Timer()
        metric = d2l.Accumulator(2)  # 训练损失总和，词元数量
        for batch in data_iter:
            optimizer.zero_grad()
            X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch]
            bos = torch.tensor([tgt_vocab['<bos>']] * Y.shape[0],
                          device=device).reshape(-1, 1)
            dec_input = torch.cat([bos, Y[:, :-1]], 1)  # 强制教学
            Y_hat, _ = net(X, dec_input, X_valid_len)
            l = loss(Y_hat, Y, Y_valid_len)
            l.sum().backward()      # 损失函数的标量进行“反向传播”
            d2l.grad_clipping(net, 1)
            num_tokens = Y_valid_len.sum()
            optimizer.step()
            with torch.no_grad():
                metric.add(l.sum(), num_tokens)
        if (epoch + 1) % 10 == 0:
            animator.add(epoch + 1, (metric[0] / metric[1],))
    print(f'loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} '
        f'tokens/sec on {str(device)}')
```

 现在，在机器翻译数据集上，我们可以 创建和训练一个循环神经网络“编码器－解码器”模型用于序列到序列的学习。 

```python
embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1
batch_size, num_steps = 64, 10
lr, num_epochs, device = 0.005, 300, d2l.try_gpu()

train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)
encoder = Seq2SeqEncoder(len(src_vocab), embed_size, num_hiddens, num_layers,
                        dropout)
decoder = Seq2SeqDecoder(len(tgt_vocab), embed_size, num_hiddens, num_layers,
                        dropout)
net = d2l.EncoderDecoder(encoder, decoder)
train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)
```

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-307.png)



## 预测

 为了采用一个接着一个词元的方式预测输出序列， 每个解码器当前时间步的输入都将来自于前一时间步的预测词元。 与训练类似，序列开始词元（`<bos>`） 在初始时间步被输入到解码器中。 该预测过程如下所示， 当输出序列的预测遇到序列结束词元（`<eos>`）时，预测就结束了。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-306.png)

```python
#@save
def predict_seq2seq(net, src_sentence, src_vocab, tgt_vocab, num_steps,
                    device, save_attention_weights=False):
    """序列到序列模型的预测"""
    # 在预测时将net设置为评估模式
    net.eval()
    src_tokens = src_vocab[src_sentence.lower().split(' ')] + [
        src_vocab['<eos>']]
    enc_valid_len = torch.tensor([len(src_tokens)], device=device)
    src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab['<pad>'])
    # 添加批量轴
    enc_X = torch.unsqueeze(
        torch.tensor(src_tokens, dtype=torch.long, device=device), dim=0)
    enc_outputs = net.encoder(enc_X, enc_valid_len)
    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)
    # 添加批量轴
    dec_X = torch.unsqueeze(torch.tensor(
        [tgt_vocab['<bos>']], dtype=torch.long, device=device), dim=0)
    output_seq, attention_weight_seq = [], []
    for _ in range(num_steps):
        Y, dec_state = net.decoder(dec_X, dec_state)
        # 我们使用具有预测最高可能性的词元，作为解码器在下一时间步的输入
        dec_X = Y.argmax(dim=2)
        pred = dec_X.squeeze(dim=0).type(torch.int32).item()
        # 保存注意力权重（稍后讨论）
        if save_attention_weights:
            attention_weight_seq.append(net.decoder.attention_weights)
        # 一旦序列结束词元被预测，输出序列的生成就完成了
        if pred == tgt_vocab['<eos>']:
            break
        output_seq.append(pred)
    return ' '.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq
```

## 预测序列的评估预测序列的评估

 我们可以通过与真实的标签序列进行比较来评估预测序列。 虽然 ([Papineni *et al.*, 2002](https://zh-v2.d2l.ai/chapter_references/zreferences.html#id118)) 提出的BLEU（bilingual evaluation understudy） 最先是用于评估机器翻译的结果， 但现在它已经被广泛用于测量许多应用的输出序列的质量。 原则上说，对于预测序列中的任意 $n$ 元语法（n-grams）， BLEU的评估都是这个 $n$ 元语法是否出现在标签序列中。 

 我们将BLEU定义为： 
$$
\exp\left(\min\left(0, 1 - \frac{\mathrm{len}_{\text{label}}}{\mathrm{len}_{\text{pred}}}\right)\right) \prod_{n=1}^k p_n^{1/2^n},\tag{9.6.4}
$$
 其中 $\mathrm{len}_{\text{label}}$ 表示标签序列中的词元数和 $\mathrm{len}_{\text{pred}}$ 表示预测序列中的词元数， $k$ 是用于匹配的最长的 $n$ 元语法。 另外，用 $p_n$ 表示 $n$ 元语法的精确度，它是两个数量的比值： 第一个是预测序列与标签序列中匹配的 $n$ 元语法的数量， 第二个是预测序列中 $n$ 元语法的数量的比率。 具体地说，给定标签序列 $A、B、C、D、E、F$ 和预测序列 $A、B、C、D$， 我们有 $p_1 = 4/5$、$p_2 = 3/4$、$p_3 = 1/3$和 $p_4 = 0$。 

 根据 $(9.7.4)$ 中BLEU的定义， 当预测序列与标签序列完全相同时，BLEU为1。 此外，由于 $n$ 元语法越长则匹配难度越大， 所以BLEU为更长的 $n$ 元语法的精确度分配更大的权重。 具体来说，当 $p_n$ 固定时，$p_n^{1/2^n}$ 会随着 $n$ 的增长而增加。 而且，由于预测的序列越短获得的 $p_n$ 值越高， 所以 $(9.7.4)$ 中乘法项之前的系数用于惩罚较短的预测序列。 例如，当 $k=2$ 时，给定标签序列  $A、B、C、D、E、F$ 和预测序列 $A、B$，尽管 $p_1 = p_2 = 1$， 惩罚因子 $\exp(1-6/2) \approx 0.14$ 会降低BLEU。 

 BLEU的代码实现如下 

```python
def bleu(pred_seq, label_seq, k):  #@save
    """计算BLEU"""
    pred_tokens, label_tokens = pred_seq.split(' '), label_seq.split(' ')
    len_pred, len_label = len(pred_tokens), len(label_tokens)
    score = math.exp(min(0, 1 - len_label / len_pred))
    for n in range(1, k + 1):
        num_matches, label_subs = 0, collections.defaultdict(int)
        for i in range(len_label - n + 1):
            label_subs[' '.join(label_tokens[i: i + n])] += 1
        for i in range(len_pred - n + 1):
            if label_subs[' '.join(pred_tokens[i: i + n])] > 0:
                num_matches += 1
                label_subs[' '.join(pred_tokens[i: i + n])] -= 1
        score *= math.pow(num_matches / (len_pred - n + 1), math.pow(0.5, n))
    return score
```

 最后，利用训练好的循环神经网络“编码器－解码器”模型， 将几个英语句子翻译成法语，并计算BLEU的最终结果。 

```python
engs = ['go .', "i lost .", 'he\'s calm .', 'i\'m home .']
fras = ['va !', 'j\'ai perdu .', 'il est calme .', 'je suis chez moi .']
for eng, fra in zip(engs, fras):
    translation, attention_weight_seq = predict_seq2seq(
        net, eng, src_vocab, tgt_vocab, num_steps, device)
    print(f'{eng} => {translation}, bleu {bleu(translation, fra, k=2):.3f}')
```

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-308.png)

## 小结

- 根据“编码器-解码器”架构的设计， 我们可以使用两个循环神经网络来设计一个序列到序列学习的模型。
- 在实现编码器和解码器时，我们可以使用多层循环神经网络。
- 我们可以使用遮蔽来过滤不相关的计算，例如在计算损失时。
- 在“编码器－解码器”训练中，强制教学方法将原始输出序列（而非预测结果）输入解码器。
- BLEU是一种常用的评估方法，它通过测量预测序列和标签序列之间的 $n$ 元语法的匹配度来评估预测。