---
title: DL-MULI-10-注意力机制
date: 2023-09-27 19:04:47
tags: 
  - DL
categories: 
  - Science
---

# 注意力提示

## 生物学中的注意力提示

 非自主性提示是基于环境中物体的突出性和易见性。 就像下图所有纸制品都是黑白印刷的，但咖啡杯是红色的。 换句话说，这个咖啡杯在这种视觉环境中是突出和显眼的， 不由自主地引起人们的注意。 

>  由于突出性的非自主性提示（红杯子），注意力不自主地指向了咖啡杯 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-309.png)



 喝咖啡后，我们会变得兴奋并想读书， 所以转过头，重新聚焦眼睛，然后看看书， 就像下图描述那样。 与上图由于突出性导致的选择不同， 此时选择书是受到了认知和意识的控制， 因此注意力在基于自主性提示去辅助选择时将更为谨慎。 受试者的主观意愿推动，选择的力量也就更强大。

>  依赖于任务的意志提示（想读一本书），注意力被自主引导到书上

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-310.png)

 

## 查询、键和值

 在注意力机制的背景下，自主性提示被称为`查询`（query）。 给定任何查询，注意力机制通过`注意力汇聚`（attention pooling） 将选择引导至`感官输入`（sensory inputs，例如中间特征表示）。 在注意力机制中，这些感官输入被称为`值`（value） 

> 更通俗的解释，每个值都与一个`键`（key）配对， 这可以想象为感官输入的非自主提示。 
>
> 如下所示，可以通过设计注意力汇聚的方式， 便于给定的查询（自主性提示）与键（非自主性提示）进行匹配， 这将引导得出最匹配的值（感官输入）。 
>
> 图：注意力机制通过注意力汇聚将`查询`（自主性提示）和`键`（非自主性提示）结合在一起，实现对`值`（感官输入）的选择倾向 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-311.png)

## 注意力的可视化

 平均汇聚层可以被视为输入的加权平均值， 其中各输入的权重是一样的。 实际上，注意力汇聚得到的是加权平均的总和值， 其中权重是在给定的查询和不同的键之间计算得出的。 

```python
import torch
from d2l import torch as d2l
```

 为了可视化注意力权重，需要定义一个`show_heatmaps`函数。 其输入`matrices`的形状是 （要显示的行数，要显示的列数，查询的数目，键的数目）。 

```python
#@save
def show_heatmaps(matrices, xlabel, ylabel, titles=None, figsize=(2.5, 2.5),
                  cmap='Reds'):
    """显示矩阵热图"""
    d2l.use_svg_display()
    num_rows, num_cols = matrices.shape[0], matrices.shape[1]
    fig, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize,
                                 sharex=True, sharey=True, squeeze=False)
    for i, (row_axes, row_matrices) in enumerate(zip(axes, matrices)):
        for j, (ax, matrix) in enumerate(zip(row_axes, row_matrices)):
            pcm = ax.imshow(matrix.detach().numpy(), cmap=cmap)
            if i == num_rows - 1:
                ax.set_xlabel(xlabel)
            if j == 0:
                ax.set_ylabel(ylabel)
            if titles:
                ax.set_title(titles[j])
    fig.colorbar(pcm, ax=axes, shrink=0.6);
```

 下面使用一个简单的例子进行演示。 在本例子中，仅当查询和键相同时，注意力权重为1，否则为0。 

```python
attention_weights = torch.eye(10).reshape((1, 1, 10, 10))
show_heatmaps(attention_weights, xlabel='Keys', ylabel='Queries')
```

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-312.png)

## 小结

- 人类的注意力是有限的、有价值和稀缺的资源。
- 受试者使用非自主性和自主性提示有选择性地引导注意力。前者基于突出性，后者则依赖于意识。
- 注意力机制与全连接层或者汇聚层的区别源于增加的自主提示。
- 由于包含了自主性提示，注意力机制与全连接的层或汇聚层不同。
- 注意力机制通过注意力汇聚使选择偏向于值（感官输入），其中包含查询（自主性提示）和键（非自主性提示）。键和值是成对的。
- 可视化查询和键之间的注意力权重是可行的。

# 注意力汇聚：Nadaraya-Watson 核回归

 本节将介绍注意力汇聚的更多细节， 以便从宏观上了解注意力机制在实践中的运作方式 

```python
import torch
from torch import nn
from d2l import torch as d2l
```

## 生成数据集

给定的成对的“输入－输出”数据集 $\{(x_1, y_1), \ldots, (x_n, y_n)\}$ ， 如何学习 $f$ 来预测任意新输入 $x$ 的输出 $\hat{y} = f(x)$？ 

根据下面的非线性函数生成一个人工数据集， 其中加入的噪声项为 $\epsilon$：
$$
y_i = 2\sin(x_i) + x_i^{0.8} + \epsilon,\tag{10.2.1}
$$
 其中 $\epsilon$ 服从均值为0和标准差为0.5的正态分布。 在这里生成了50个训练样本和50个测试样本。 为了更好地可视化之后的注意力模式，需要将训练样本进行排序。 

```python
n_train = 50  # 训练样本数
x_train, _ = torch.sort(torch.rand(n_train) * 5)   # 排序后的训练样本

def f(x):
    return 2 * torch.sin(x) + x**0.8

y_train = f(x_train) + torch.normal(0.0, 0.5, (n_train,))  # 训练样本的输出
x_test = torch.arange(0, 5, 0.1)  # 测试样本
y_truth = f(x_test)  # 测试样本的真实输出
n_test = len(x_test)  # 测试样本数
n_test
```

 下面的函数将绘制所有的训练样本（样本由圆圈表示）， 不带噪声项的真实数据生成函数 $f$（标记为“Truth”）， 以及学习得到的预测函数（标记为“Pred”）。 

```python
def plot_kernel_reg(y_hat):
    d2l.plot(x_test, [y_truth, y_hat], 'x', 'y', legend=['Truth', 'Pred'],
             xlim=[0, 5], ylim=[-1, 5])
    d2l.plt.plot(x_train, y_train, 'o', alpha=0.5);
```

## 平均汇聚

 先使用最简单的估计器来解决回归问题。 基于平均汇聚来计算所有训练样本输出值的平均值： 
$$
f(x) = \frac{1}{n}\sum_{i=1}^n y_i,\tag{10.2.2}
$$
 如下图所示，这个估计器确实不够聪明。 真实函数 $f$（“Truth”）和预测函数（“Pred”）相差很大。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-313.png)

## 非参数注意力汇聚

 显然，平均汇聚忽略了输入 $x_i$。 我们 根据输入的位置对输出 $y_i$ 进行加权： 
$$
f(x) = \sum_{i=1}^n \frac{K(x - x_i)}{\sum_{j=1}^n K(x - x_j)} y_i,\tag{10.2.3}
$$
 其中是`核`kernel）。 公式 $(10.2.3)$所描述的估计器被称为 `Nadaraya-Watson核回归`（Nadaraya-Watson kernel regression）。 

 从注意力机制框架的角 重写$(10.2.3)$， 成为一个更加通用的`注意力汇聚`（attention pooling）公式： 
$$
f(x) = \sum_{i=1}^n \alpha(x, x_i) y_i,\tag{10.2.4}
$$
 其中 $x_i$ 是查询，$(x_i,y_i)$ 是键值对。 比较 $(10.2.4)$ 和 $(10.2.2)$， 注意力汇聚是的加权平均。 将查询 $x$ 和键 $x_i$ 之间的关系建模为`注意力权重`（attention weight）$\alpha(x, x_i)$， 如$(10.2.4)$ 所示， 这个权重将被分配给每一个对应值 $y_i$。 

对于任何查询，模型在所有键值对注意力权重都是一个有效的概率分布： 它们是非负的，并且总和为1。 

 为了更好地理解注意力汇聚， 下面考虑一个`高斯核`（Gaussian kernel），其定义为 
$$
K(u) = \frac{1}{\sqrt{2\pi}} \exp(-\frac{u^2}{2}).\tag{10.2.5}
$$
 将高斯核代入$(10.2.4)$ 和 $(10.2.3)$ 可以得到： 
$$
\begin{split}\begin{aligned} f(x) &=\sum_{i=1}^n \alpha(x, x_i) y_i\\ &= \sum_{i=1}^n \frac{\exp\left(-\frac{1}{2}(x - x_i)^2\right)}{\sum_{j=1}^n \exp\left(-\frac{1}{2}(x - x_j)^2\right)} y_i \\&= \sum_{i=1}^n \mathrm{softmax}\left(-\frac{1}{2}(x - x_i)^2\right) y_i. \end{aligned}\end{split}\tag{10.2.6}
$$
 在 $(10.2.6)$ 中， 如果一个键 $x_i$ 越是接近给定的查询 $x$， 那么分配给这个键对应值 $y_i$ 的注意力权重就会越大， 也就“获得了更多的注意力”。 

 值得注意的是，Nadaraya-Watson核回归是一个非参数模型。 因此， $(10.2.6)$ 是 `非参数的注意力汇聚`（nonparametric attention pooling）模型 

 接下来，我们将基于这个非参数的注意力汇聚模型来绘制预测结果。 从绘制的结果会发现新的模型预测线是平滑的，并且比平均汇聚的预测更接近真实。 

````python
# X_repeat的形状:(n_test,n_train),
# 每一行都包含着相同的测试输入（例如：同样的查询）
X_repeat = x_test.repeat_interleave(n_train).reshape((-1, n_train))
# x_train包含着键。attention_weights的形状：(n_test,n_train),
# 每一行都包含着要在给定的每个查询的值（y_train）之间分配的注意力权重
attention_weights = nn.functional.softmax(-(X_repeat - x_train)**2 / 2, dim=1)
# y_hat的每个元素都是值的加权平均值，其中的权重是注意力权重
y_hat = torch.matmul(attention_weights, y_train)
plot_kernel_reg(y_hat)
````

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-314.png)

 现在来观察注意力的权重。 这里测试数据的输入相当于查询，而训练数据的输入相当于键。 因为两个输入都是经过排序的，因此由观察可知“查询-键”对越接近， 注意力汇聚的注意力权重就越高。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-315.png)

## 带参数注意力汇聚

 非参数的Nadaraya-Watson核回归具有`一致性`（consistency）的优点：

 如果有足够的数据，此模型会收敛到最优结果。 尽管如此，我们还是可以轻松地将可学习的参数集成到注意力汇聚中。 

 例如，与$(10.2.6)$ 略有不同， 在下面的查询 $x$ 和键 $x_i$ 之间的距离乘以可学习参数 $w$
$$
\begin{split}\begin{aligned}f(x) &= \sum_{i=1}^n \alpha(x, x_i) y_i \\&= \sum_{i=1}^n \frac{\exp\left(-\frac{1}{2}((x - x_i)w)^2\right)}{\sum_{j=1}^n \exp\left(-\frac{1}{2}((x - x_j)w)^2\right)} y_i \\&= \sum_{i=1}^n \mathrm{softmax}\left(-\frac{1}{2}((x - x_i)w)^2\right) y_i.\end{aligned}\end{split}\tag{10.2.7}
$$

### 批量矩阵乘法

 为了更有效地计算小批量数据的注意力， 我们可以利用深度学习开发框架中提供的批量矩阵乘法。 

 假设第一个小批量数据包含n个矩阵 $\mathbf{X}_1,\ldots, \mathbf{X}_n$， 形状为 $a\times b$， 第二个小批量包含n个矩阵， 形状为 $\mathbf{Y}_1, \ldots, \mathbf{Y}_n$。 它们的批量矩阵乘法得到n个矩阵 $\mathbf{X}_1\mathbf{Y}_1, \ldots, \mathbf{X}_n\mathbf{Y}_n$， 形状为 $b\times c$。 因此，假定两个张量的形状分别是 $(n,a,b)$ 和 $(n,b,c)$， 它们的批量矩阵乘法输出的形状为 $(n,a,c)$。 

```python
X = torch.ones((2, 1, 4))
Y = torch.ones((2, 4, 6))
torch.bmm(X, Y).shape
```

 在注意力机制的背景中，我们可以使用小批量矩阵乘法来计算小批量数据中的加权平均值。 

```python
weights = torch.ones((2, 10)) * 0.1
values = torch.arange(20.0).reshape((2, 10))
torch.bmm(weights.unsqueeze(1), values.unsqueeze(-1))
```

### 定义模型

 基于$(10.2.7)$ 中的 带参数的注意力汇聚，使用小批量矩阵乘法， 定义Nadaraya-Watson核回归的带参数版本为： 

```python
class NWKernelRegression(nn.Module):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.w = nn.Parameter(torch.rand((1,), requires_grad=True))

    def forward(self, queries, keys, values):
        # queries和attention_weights的形状为(查询个数，“键－值”对个数)
        queries = queries.repeat_interleave(keys.shape[1]).reshape((-1, keys.shape[1]))
        self.attention_weights = nn.functional.softmax(
            -((queries - keys) * self.w)**2 / 2, dim=1)
        # values的形状为(查询个数，“键－值”对个数)
        return torch.bmm(self.attention_weights.unsqueeze(1),
                         values.unsqueeze(-1)).reshape(-1)
```

### 训练

 接下来，将训练数据集变换为键和值用于训练注意力模型。 在带参数的注意力汇聚模型中， 任何一个训练样本的输入都会和除自己以外的所有训练样本的“键－值”对进行计算， 从而得到其对应的预测输出。 

```python
# X_tile的形状:(n_train，n_train)，每一行都包含着相同的训练输入
X_tile = x_train.repeat((n_train, 1))
# Y_tile的形状:(n_train，n_train)，每一行都包含着相同的训练输出
Y_tile = y_train.repeat((n_train, 1))
# keys的形状:('n_train'，'n_train'-1)
keys = X_tile[(1 - torch.eye(n_train)).type(torch.bool)].reshape((n_train, -1))
# values的形状:('n_train'，'n_train'-1)
values = Y_tile[(1 - torch.eye(n_train)).type(torch.bool)].reshape((n_train, -1))
```

 训练带参数的注意力汇聚模型时，使用平方损失函数和随机梯度下降。 

```python
net = NWKernelRegression()
loss = nn.MSELoss(reduction='none')
trainer = torch.optim.SGD(net.parameters(), lr=0.5)
animator = d2l.Animator(xlabel='epoch', ylabel='loss', xlim=[1, 5])

for epoch in range(5):
    trainer.zero_grad()
    l = loss(net(x_train, keys, values), y_train)
    l.sum().backward()
    trainer.step()
    print(f'epoch {epoch + 1}, loss {float(l.sum()):.6f}')
    animator.add(epoch + 1, float(l.sum()))
```

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-316.png)

 如下所示，训练完带参数的注意力汇聚模型后可以发现： 在尝试拟合带噪声的训练数据时， 预测结果绘制的线不如之前非参数模型的平滑。 

```python
# keys的形状:(n_test，n_train)，每一行包含着相同的训练输入（例如，相同的键）
keys = x_train.repeat((n_test, 1))
# value的形状:(n_test，n_train)
values = y_train.repeat((n_test, 1))
y_hat = net(x_test, keys, values).unsqueeze(1).detach()
plot_kernel_reg(y_hat)
```

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-317.png)

## 小结

- Nadaraya-Watson核回归是具有注意力机制的机器学习范例。
- Nadaraya-Watson核回归的注意力汇聚是对训练数据中输出的加权平均。从注意力的角度来看，分配给每个值的注意力权重取决于将值所对应的键和查询作为输入的函数。
- 注意力汇聚可以分为非参数型和带参数型

 

# 注意力评分函数

$$
\begin{split}\begin{aligned} f(x) &=\sum_{i=1}^n \alpha(x, x_i) y_i\\ &= \sum_{i=1}^n \frac{\exp\left(-\frac{1}{2}(x - x_i)^2\right)}{\sum_{j=1}^n \exp\left(-\frac{1}{2}(x - x_j)^2\right)} y_i \\&= \sum_{i=1}^n \mathrm{softmax}\left(-\frac{1}{2}(x - x_i)^2\right) y_i. \end{aligned}\end{split}\tag{10.2.6}
$$

上小节使用了高斯核来对查询和键之间的关系建模。 $(10.2.6)$ 中的 高斯核指数部分可以视为`注意力评分函数`（attention scoring function）， 简称`评分函数`（scoring function）。

 然后把这个函数的输出结果输入到softmax函数中进行运算。通过上述步骤，将得到与键对应的值的概率分布（即注意力权重）。 最后，注意力汇聚的输出就是基于这些注意力权重的值的加权和。 

> 下图说明了 如何将注意力汇聚的输出计算成为值的加权和， 其中 $a$ 表示注意力评分函数。 由于注意力权重是概率分布， 因此加权和其本质上是加权平均值。 
>
> 图： 计算注意力汇聚的输出为值的加权和 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-318.png)

 用数学语言描述，假设有一个查询 $\mathbf{q} \in \mathbb{R}^q$ 和 $m$ 个“键－值”对 $(\mathbf{k}_1, \mathbf{v}_1), \ldots, (\mathbf{k}_m, \mathbf{v}_m)$， 其中 $\mathbf{k}_i \in \mathbb{R}^k$，$\mathbf{v}_i \in \mathbb{R}^v$。 注意力汇聚函数  $f$ 就被表示成值的加权和： 
$$
f(\mathbf{q}, (\mathbf{k}_1, \mathbf{v}_1), \ldots, (\mathbf{k}_m, \mathbf{v}_m)) = \sum_{i=1}^m \alpha(\mathbf{q}, \mathbf{k}_i) \mathbf{v}_i \in \mathbb{R}^v,\tag{10.3.1}
$$
 其中查询 $\mathbf{q}$ 和键 $\mathbf{k}_i$ 的注意力权重（标量） 是通过注意力评分函数 $a$ 将两个向量映射成标量， 再经过softmax运算得到的 
$$
\alpha(\mathbf{q}, \mathbf{k}_i) = \mathrm{softmax}(a(\mathbf{q}, \mathbf{k}_i)) = \frac{\exp(a(\mathbf{q}, \mathbf{k}_i))}{\sum_{j=1}^m \exp(a(\mathbf{q}, \mathbf{k}_j))} \in \mathbb{R}.\tag{10.3.2}
$$
 正如上图所示，选择不同的注意力评分函数 $a$ 会导致不同的注意力汇聚操作。 

```python
import math
import torch
from torch import nn
from d2l import torch as d2l
```

## 掩蔽softmax操作

 正如上面提到的，softmax操作用于输出一个概率分布作为注意力权重。 在某些情况下，并非所有的值都应该被纳入到注意力汇聚中。  

为了仅将有意义的词元作为值来获取注意力汇聚， 可以指定一个有效序列长度（即词元的个数）， 以便在计算softmax时过滤掉超出指定范围的位置。 

 下面的`masked_softmax`函数 实现了这样的`掩蔽softmax操作`（masked softmax operation）， 其中任何超出有效长度的位置都被掩蔽并置为0。 

```python
#@save
def masked_softmax(X, valid_lens):
    """通过在最后一个轴上掩蔽元素来执行softmax操作"""
    # X:3D张量，valid_lens:1D或2D张量
    if valid_lens is None:
        return nn.functional.softmax(X, dim=-1)
    else:
        shape = X.shape
        if valid_lens.dim() == 1:
            valid_lens = torch.repeat_interleave(valid_lens, shape[1])
        else:
            valid_lens = valid_lens.reshape(-1)
        # 最后一轴上被掩蔽的元素使用一个非常大的负值替换，从而其softmax输出为0
        X = d2l.sequence_mask(X.reshape(-1, shape[-1]), valid_lens,
                              value=-1e6)
        return nn.functional.softmax(X.reshape(shape), dim=-1)
```

 为了演示此函数是如何工作的， 考虑由两个$2 \times 4$矩阵表示的样本， 这两个样本的有效长度分别为2和3。 经过掩蔽softmax操作，超出有效长度的值都被掩蔽为0。 

```python
masked_softmax(torch.rand(2, 2, 4), torch.tensor([2, 3]))
```

 同样，也可以使用二维张量，为矩阵样本中的每一行指定有效长度 

```python
masked_softmax(torch.rand(2, 2, 4), torch.tensor([[1, 3], [2, 4]]))
```

## 加性注意力

 一般来说，当查询和键是不同长度的矢量时，可以使用加性注意力作为评分函数。 给定查询 $\mathbf{q} \in \mathbb{R}^q$和 键 $\mathbf{k} \in \mathbb{R}^k$， `加性注意力`（additive attention）的评分函数为 
$$
a(\mathbf q, \mathbf k) = \mathbf w_v^\top \text{tanh}(\mathbf W_q\mathbf q + \mathbf W_k \mathbf k) \in \mathbb{R},\tag{10.3.3}
$$
 其中可学习的参数是 $\mathbf W_q\in\mathbb R^{h\times q}$、$\mathbf W_k\in\mathbb R^{h\times k}$ 和 $\mathbf w_v\in\mathbb R^{h}$。 

 如 $(10.3.3)$ 所示， 将查询和键连结起来后输入到一个多层感知机（MLP）中， 感知机包含一个隐藏层，其隐藏单元数是一个超参数 $h$。 通过使用 $\tanh$ 作为激活函数，并且禁用偏置项。 

 下面来实现加性注意力。 

```python
#@save
class AdditiveAttention(nn.Module):
    """加性注意力"""
    def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs):
        super(AdditiveAttention, self).__init__(**kwargs)
        self.W_k = nn.Linear(key_size, num_hiddens, bias=False)
        self.W_q = nn.Linear(query_size, num_hiddens, bias=False)
        self.w_v = nn.Linear(num_hiddens, 1, bias=False)
        self.dropout = nn.Dropout(dropout)

    def forward(self, queries, keys, values, valid_lens):
        queries, keys = self.W_q(queries), self.W_k(keys)
        # 在维度扩展后，
        # queries的形状：(batch_size，查询的个数，1，num_hidden)
        # key的形状：(batch_size，1，“键－值”对的个数，num_hiddens)
        # 使用广播方式进行求和
        features = queries.unsqueeze(2) + keys.unsqueeze(1)
        features = torch.tanh(features)
        # self.w_v仅有一个输出，因此从形状中移除最后那个维度。
        # scores的形状：(batch_size，查询的个数，“键-值”对的个数)
        scores = self.w_v(features).squeeze(-1)
        self.attention_weights = masked_softmax(scores, valid_lens)
        # values的形状：(batch_size，“键－值”对的个数，值的维度)
        return torch.bmm(self.dropout(self.attention_weights), values)
```

 用一个小例子来演示上面的`AdditiveAttention`类， 其中查询、键和值的形状为`（批量大小，步数或词元序列长度，特征大小）`， 实际输出为 $(2,1,20)$、$(2,10,2)$和 $(2,10,4)$。 注意力汇聚输出的形状为`（批量大小，查询的步数，值的维度）`。 

```python
queries, keys = torch.normal(0, 1, (2, 1, 20)), torch.ones((2, 10, 2))
# values的小批量，两个值矩阵是相同的
values = torch.arange(40, dtype=torch.float32).reshape(1, 10, 4).repeat(
    2, 1, 1)
valid_lens = torch.tensor([2, 6])

attention = AdditiveAttention(key_size=2, query_size=20, num_hiddens=8,
                              dropout=0.1)
attention.eval()
attention(queries, keys, values, valid_lens)
```

 尽管加性注意力包含了可学习的参数，但由于本例子中每个键都是相同的， 所以注意力权重是均匀的，由指定的有效长度决定。 

```python
d2l.show_heatmaps(attention.attention_weights.reshape((1, 1, 2, 10)),
                  xlabel='Keys', ylabel='Queries')
```

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-319.png)

## 缩放点积注意力

 使用点积可以得到计算效率更高的评分函数， 但是点积操作要求查询和键具有相同的长度 $d$。 假设查询和键的所有元素都是独立的随机变量， 并且都满足零均值和单位方差， 那么两个向量的点积的均值为 $0$，方差为 $d$。 为确保无论向量长度如何， 点积的方差在不考虑向量长度的情况下仍然是 $1$， 我们再将点积除以 $\sqrt{d}$， 则`缩放点积注意力`（scaled dot-product attention）评分函数为： 
$$
a(\mathbf q, \mathbf k) = \mathbf{q}^\top \mathbf{k}  /\sqrt{d}.\tag{10.3.4}
$$
 在实践中，我们通常从小批量的角度来考虑提高效率， 例如基于$n$个查询和 $m$个键－值对计算注意力， 其中查询和键的长度为  $d$，值的长度为 $v$。  查询 $\mathbf Q\in\mathbb R^{n\times d}$、 键 $\mathbf K\in\mathbb R^{m\times d}$和 值 $\mathbf V\in\mathbb R^{m\times v}$的缩放点积注意力是： 
$$
\mathrm{softmax}\left(\frac{\mathbf Q \mathbf K^\top }{\sqrt{d}}\right) \mathbf V \in \mathbb{R}^{n\times v}.\tag{10.3.5}
$$
 下面的缩放点积注意力的实现使用了暂退法进行模型正则化。 

```python
#@save
class DotProductAttention(nn.Module):
    """缩放点积注意力"""
    def __init__(self, dropout, **kwargs):
        super(DotProductAttention, self).__init__(**kwargs)
        self.dropout = nn.Dropout(dropout)

    # queries的形状：(batch_size，查询的个数，d)
    # keys的形状：(batch_size，“键－值”对的个数，d)
    # values的形状：(batch_size，“键－值”对的个数，值的维度)
    # valid_lens的形状:(batch_size，)或者(batch_size，查询的个数)
    def forward(self, queries, keys, values, valid_lens=None):
        d = queries.shape[-1]
        # 设置transpose_b=True为了交换keys的最后两个维度
        scores = torch.bmm(queries, keys.transpose(1,2)) / math.sqrt(d)
        self.attention_weights = masked_softmax(scores, valid_lens)
        return torch.bmm(self.dropout(self.attention_weights), values)
```

 为了演示上述的`DotProductAttention`类， 我们使用与先前加性注意力例子中相同的键、值和有效长度。 对于点积操作，我们令查询的特征维度与键的特征维度大小相同。 

```python
queries = torch.normal(0, 1, (2, 1, 2))
attention = DotProductAttention(dropout=0.5)
attention.eval()
attention(queries, keys, values, valid_lens)
```

 与加性注意力演示相同，由于键包含的是相同的元素， 而这些元素无法通过任何查询进行区分，因此获得了均匀的注意力权重。 

```python
d2l.show_heatmaps(attention.attention_weights.reshape((1, 1, 2, 10)),
                  xlabel='Keys', ylabel='Queries')
```

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-320.png)

## 小结

- 注意力分数是query和key的相似度，注意力权重是分数的softmax结果
- 两种常见的分数计算：
  * 将query和key合并起来进入单输出单隐藏层的MLP
  * 直接将query和key做内积

# Bahdanau 注意力

之前探讨了机器翻译问题： 通过设计一个基于两个循环神经网络的编码器-解码器架构， 用于序列到序列学习。 具体来说，循环神经网络编码器将长度可变的序列转换为固定形状的上下文变量， 然后循环神经网络解码器根据生成的词元和上下文变量 按词元生成输出（目标）序列词元。 然而，即使并非所有输入（源）词元都对解码某个词元都有用， 在每个解码步骤中仍使用编码`相同`的上下文变量。 有什么方法能改变上下文变量呢？

我们试着从 ([Graves, 2013](https://zh-v2.d2l.ai/chapter_references/zreferences.html#id50))中找到灵感： 在为给定文本序列生成手写的挑战中， Graves设计了一种可微注意力模型， 将文本字符与更长的笔迹对齐， 其中对齐方式仅向一个方向移动。 

受学习对齐想法的启发，Bahdanau等人提出了一个没有严格单向对齐限制的 可微注意力模型 ([Bahdanau *et al.*, 2014](https://zh-v2.d2l.ai/chapter_references/zreferences.html#id6))。 在预测词元时，如果不是所有输入词元都相关，模型将仅对齐（或参与）输入序列中与当前预测相关的部分。这是通过将上下文变量视为注意力集中的输出来实现的。

##  模型

 假设输入序列中有 $T$ 个词元， 解码时间步 $t'$ 的上下文变量是注意力集中的输出： 
$$
\mathbf{c}_{t'} = \sum_{t=1}^T \alpha(\mathbf{s}_{t' - 1}, \mathbf{h}_t) \mathbf{h}_t,\tag{10.4.1}
$$
 其中，时间步 $t' - 1$ 时的解码器隐状态$\mathbf{s}_{t' - 1}$是查询， 编码器隐状态$\mathbf{h}_t$既是键，也是值， 注意力权重$\alpha$是使用 $(10.3.2)$ 所定义的加性注意力打分函数计算的。 

> 下图描述了Bahdanau注意力的架构
>
> 图： 一个带有Bahdanau注意力的循环神经网络编码器-解码器模型 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-321.png)

```python
import torch
from torch import nn
from d2l import torch as d2l
```

## 定义注意力解码器

 下面看看如何定义Bahdanau注意力，实现循环神经网络编码器-解码器。 其实，我们只需重新定义解码器即可。 为了更方便地显示学习的注意力权重， 以下`AttentionDecoder`类定义了带有注意力机制解码器的基本接口 

```python
#@save
class AttentionDecoder(d2l.Decoder):
    """带有注意力机制解码器的基本接口"""
    def __init__(self, **kwargs):
        super(AttentionDecoder, self).__init__(**kwargs)

    @property
    def attention_weights(self):
        raise NotImplementedError
```

 接下来，让我们在接下来的`Seq2SeqAttentionDecoder`类中 实现带有Bahdanau注意力的循环神经网络解码器。 首先，初始化解码器的状态，需要下面的输入： 

1. 编码器在所有时间步的最终层隐状态，将作为注意力的键和值；
2. 上一时间步的编码器全层隐状态，将作为初始化解码器的隐状态；
3. 编码器有效长度（排除在注意力池中填充词元）。

 在每个解码时间步骤中，解码器上一个时间步的最终层隐状态将用作查询。 因此，注意力输出和输入嵌入都连结为循环神经网络解码器的输入。 

```python
class Seq2SeqAttentionDecoder(AttentionDecoder):
    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
                 dropout=0, **kwargs):
        super(Seq2SeqAttentionDecoder, self).__init__(**kwargs)
        self.attention = d2l.AdditiveAttention(
            num_hiddens, num_hiddens, num_hiddens, dropout)
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.rnn = nn.GRU(
            embed_size + num_hiddens, num_hiddens, num_layers,
            dropout=dropout)
        self.dense = nn.Linear(num_hiddens, vocab_size)

    def init_state(self, enc_outputs, enc_valid_lens, *args):
        # outputs的形状为(batch_size，num_steps，num_hiddens).
        # hidden_state的形状为(num_layers，batch_size，num_hiddens)
        outputs, hidden_state = enc_outputs
        return (outputs.permute(1, 0, 2), hidden_state, enc_valid_lens)

    def forward(self, X, state):
        # enc_outputs的形状为(batch_size,num_steps,num_hiddens).
        # hidden_state的形状为(num_layers,batch_size,
        # num_hiddens)
        enc_outputs, hidden_state, enc_valid_lens = state
        # 输出X的形状为(num_steps,batch_size,embed_size)
        X = self.embedding(X).permute(1, 0, 2)
        outputs, self._attention_weights = [], []
        for x in X:
            # query的形状为(batch_size,1,num_hiddens)
            query = torch.unsqueeze(hidden_state[-1], dim=1)
            # context的形状为(batch_size,1,num_hiddens)
            context = self.attention(
                query, enc_outputs, enc_outputs, enc_valid_lens)
            # 在特征维度上连结
            x = torch.cat((context, torch.unsqueeze(x, dim=1)), dim=-1)
            # 将x变形为(1,batch_size,embed_size+num_hiddens)
            out, hidden_state = self.rnn(x.permute(1, 0, 2), hidden_state)
            outputs.append(out)
            self._attention_weights.append(self.attention.attention_weights)
        # 全连接层变换后，outputs的形状为
        # (num_steps,batch_size,vocab_size)
        outputs = self.dense(torch.cat(outputs, dim=0))
        return outputs.permute(1, 0, 2), [enc_outputs, hidden_state,
                                          enc_valid_lens]

    @property
    def attention_weights(self):
        return self._attention_weights
```

 接下来，使用包含7个时间步的4个序列输入的小批量测试Bahdanau注意力解码器。 

```python
encoder = d2l.Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16,
                             num_layers=2)
encoder.eval()
decoder = Seq2SeqAttentionDecoder(vocab_size=10, embed_size=8, num_hiddens=16,
                                  num_layers=2)
decoder.eval()
X = torch.zeros((4, 7), dtype=torch.long)  # (batch_size,num_steps)
state = decoder.init_state(encoder(X), None)
output, state = decoder(X, state)
output.shape, len(state), state[0].shape, len(state[1]), state[1][0].shape
```

## 训练

 我们在这里指定超参数，实例化一个带有Bahdanau注意力的编码器和解码器， 并对这个模型进行机器翻译训练。  

```python
embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1
batch_size, num_steps = 64, 10
lr, num_epochs, device = 0.005, 250, d2l.try_gpu()

train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)
encoder = d2l.Seq2SeqEncoder(
    len(src_vocab), embed_size, num_hiddens, num_layers, dropout)
decoder = Seq2SeqAttentionDecoder(
    len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)
net = d2l.EncoderDecoder(encoder, decoder)
d2l.train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)
```

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-322.png)

 模型训练后，我们用它将几个英语句子翻译成法语并计算它们的BLEU分数。 

```python
engs = ['go .', "i lost .", 'he\'s calm .', 'i\'m home .']
fras = ['va !', 'j\'ai perdu .', 'il est calme .', 'je suis chez moi .']
for eng, fra in zip(engs, fras):
    translation, dec_attention_weight_seq = d2l.predict_seq2seq(
        net, eng, src_vocab, tgt_vocab, num_steps, device, True)
    print(f'{eng} => {translation}, ',
          f'bleu {d2l.bleu(translation, fra, k=2):.3f}')
```

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-323.png)

```python
attention_weights = torch.cat([step[0][0][0] for step in dec_attention_weight_seq], 0).reshape((
    1, 1, -1, num_steps))
```

 训练结束后，下面通过可视化注意力权重 会发现，每个查询都会在键值对上分配不同的权重，这说明 在每个解码步中，输入序列的不同部分被选择性地聚集在注意力池中。 

```python
# 加上一个包含序列结束词元
d2l.show_heatmaps(
    attention_weights[:, :, :, :len(engs[-1].split()) + 1].cpu(),
    xlabel='Key positions', ylabel='Query positions')
```

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-324.png)

## 小结

- 在预测词元时，如果不是所有输入词元都是相关的，那么具有Bahdanau注意力的循环神经网络编码器-解码器会有选择地统计输入序列的不同部分。这是通过将上下文变量视为加性注意力池化的输出来实现的。
- 在循环神经网络编码器-解码器中，Bahdanau注意力将上一时间步的解码器隐状态视为查询，在所有时间步的编码器隐状态同时视为键和值。

# 多头注意力

 在实践中，当给定相同的查询、键和值的集合时， 我们希望模型可以基于相同的注意力机制学习到不同的行为， 然后将不同的行为作为知识组合起来， 捕获序列内各种范围的依赖关系 （例如，短距离依赖和长距离依赖关系）。 因此，允许注意力机制组合使用查询、键和值的不同 子空间表示（representation subspaces）可能是有益的。 

* 我们可以用独立学习得到的 $h$组不同的 `线性投影`（linear projections）来变换查询、键和值。
*  然后，这 $h$ 组变换后的查询、键和值将并行地送到注意力汇聚中。 
* 最后，将这 $h$ 个注意力汇聚的输出拼接在一起， 并且通过另一个可以学习的线性投影进行变换， 以产生最终输出。  

 这种设计被称为`多头注意力`（multihead attention） ([Vaswani *et al.*, 2017](https://zh-v2.d2l.ai/chapter_references/zreferences.html#id174))。 对于个注意力汇聚输出，每一个注意力汇聚都被称作一个`头`（head）。

> 下图展示了使用全连接层来实现可学习的线性变换的多头注意力。 
>
> 图： 多头注意力--多个头连结然后线性变换 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-325.png)

##  模型

 给定查询 $\mathbf{q} \in \mathbb{R}^{d_q}$、 键 $\mathbf{k} \in \mathbb{R}^{d_k}$和 值 $\mathbf{v} \in \mathbb{R}^{d_v}$， 每个注意力头 $\mathbf{h}_i$  （$i = 1, \ldots, h$） 的计算方法为： 
$$
\mathbf{h}_i = f(\mathbf W_i^{(q)}\mathbf q, \mathbf W_i^{(k)}\mathbf k,\mathbf W_i^{(v)}\mathbf v) \in \mathbb R^{p_v},\tag{10.5.1}
$$
 其中，可学习的参数包括 $\mathbf W_i^{(q)}\in\mathbb R^{p_q\times d_q}$ 、 $\mathbf W_i^{(k)}\in\mathbb R^{p_k\times d_k}$ 和 $\mathbf W_i^{(v)}\in\mathbb R^{p_v\times d_v}$， 以及代表注意力汇聚的函数 $f$。  $f$ 可以是 加性注意力和缩放点积注意力。 多头注意力的输出需要经过另一个线性转换， 它对应着 $h$ 个头连结后的结果，因此其可学习参数是 $\mathbf W_o\in\mathbb R^{p_o\times h p_v}$： 
$$
\begin{split}\mathbf W_o \begin{bmatrix}\mathbf h_1\\\vdots\\\mathbf h_h\end{bmatrix} \in \mathbb{R}^{p_o}.\end{split}\tag{10.5.2}
$$
 基于这种设计，每个头都可能会关注输入的不同部分， 可以表示比简单加权平均值更复杂的函数。 

```python
import math
import torch
from torch import nn
from d2l import torch as d2l
```

## 实现

 在实现过程中通常选择缩放点积注意力作为每一个注意力头。 为了避免计算代价和参数代价的大幅增长， 我们设定 $p_q = p_k = p_v = p_o / h$。 值得注意的是，如果将查询、键和值的线性变换的输出数量设置为 $p_q h = p_k h = p_v h = p_o$， 则可以并行计算 $h$ 个头。 在下面的实现中，$p_o$ 是通过参数`num_hiddens`指定的。 

```python
#@save
class MultiHeadAttention(nn.Module):
    """多头注意力"""
    def __init__(self, key_size, query_size, value_size, num_hiddens,
                 num_heads, dropout, bias=False, **kwargs):
        super(MultiHeadAttention, self).__init__(**kwargs)
        self.num_heads = num_heads
        self.attention = d2l.DotProductAttention(dropout)
        self.W_q = nn.Linear(query_size, num_hiddens, bias=bias)
        self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)
        self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)
        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)

    def forward(self, queries, keys, values, valid_lens):
        # queries，keys，values的形状:
        # (batch_size，查询或者“键－值”对的个数，num_hiddens)
        # valid_lens　的形状:
        # (batch_size，)或(batch_size，查询的个数)
        # 经过变换后，输出的queries，keys，values　的形状:
        # (batch_size*num_heads，查询或者“键－值”对的个数，
        # num_hiddens/num_heads)
        queries = transpose_qkv(self.W_q(queries), self.num_heads)
        keys = transpose_qkv(self.W_k(keys), self.num_heads)
        values = transpose_qkv(self.W_v(values), self.num_heads)

        if valid_lens is not None:
            # 在轴0，将第一项（标量或者矢量）复制num_heads次，
            # 然后如此复制第二项，然后诸如此类。
            valid_lens = torch.repeat_interleave(
                valid_lens, repeats=self.num_heads, dim=0)

        # output的形状:(batch_size*num_heads，查询的个数，
        # num_hiddens/num_heads)
        output = self.attention(queries, keys, values, valid_lens)

        # output_concat的形状:(batch_size，查询的个数，num_hiddens)
        output_concat = transpose_output(output, self.num_heads)
        return self.W_o(output_concat)
```

 为了能够使多个头并行计算， 上面的`MultiHeadAttention`类将使用下面定义的两个转置函数。 具体来说，`transpose_output`函数反转了`transpose_qkv`函数的操作。 

```python
#@save
def transpose_qkv(X, num_heads):
    """为了多注意力头的并行计算而变换形状"""
    # 输入X的形状:(batch_size，查询或者“键－值”对的个数，num_hiddens)
    # 输出X的形状:(batch_size，查询或者“键－值”对的个数，num_heads，
    # num_hiddens/num_heads)
    X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)

    # 输出X的形状:(batch_size，num_heads，查询或者“键－值”对的个数,
    # num_hiddens/num_heads)
    X = X.permute(0, 2, 1, 3)

    # 最终输出的形状:(batch_size*num_heads,查询或者“键－值”对的个数,
    # num_hiddens/num_heads)
    return X.reshape(-1, X.shape[2], X.shape[3])


#@save
def transpose_output(X, num_heads):
    """逆转transpose_qkv函数的操作"""
    X = X.reshape(-1, num_heads, X.shape[1], X.shape[2])
    X = X.permute(0, 2, 1, 3)
    return X.reshape(X.shape[0], X.shape[1], -1)
```

 下面使用键和值相同的小例子来测试我们编写的`MultiHeadAttention`类。 多头注意力输出的形状是（`batch_size`，`num_queries`，`num_hiddens`）。 

```python
num_hiddens, num_heads = 100, 5
attention = MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens,
                               num_hiddens, num_heads, 0.5)
attention.eval()
```

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-326.png)

```python
batch_size, num_queries = 2, 4
num_kvpairs, valid_lens =  6, torch.tensor([3, 2])
X = torch.ones((batch_size, num_queries, num_hiddens))
Y = torch.ones((batch_size, num_kvpairs, num_hiddens))
attention(X, Y, Y, valid_lens).shape
```

## 小结

* 多头注意力融合了来自于多个注意力汇聚的不同知识，这些知识的不同来源于相同的查询、键和值的不同的子空间表示。

* 基于适当的张量操作，可以实现多头注意力的并行计算

# 自注意力和位置编码

 在深度学习中，经常使用卷积神经网络（CNN）或循环神经网络（RNN）对序列进行编码。 想象一下，有了注意力机制之后，我们将词元序列输入注意力池化中， **以便同一组词元同时充当查询、键和值。** 

 具体来说，每个查询都会关注所有的键－值对并生成一个注意力输出。 由于查询、键和值来自同一组输入，因此被称为`自注意力`（self-attention） ([Lin *et al.*, 2017](https://zh-v2.d2l.ai/chapter_references/zreferences.html#id94), [Vaswani *et al.*, 2017](https://zh-v2.d2l.ai/chapter_references/zreferences.html#id174))， 也被称为`内部注意力`（intra-attention） ([Cheng *et al.*, 2016](https://zh-v2.d2l.ai/chapter_references/zreferences.html#id22), [Parikh *et al.*, 2016](https://zh-v2.d2l.ai/chapter_references/zreferences.html#id119), [Paulus *et al.*, 2017](https://zh-v2.d2l.ai/chapter_references/zreferences.html#id121))。 

```python
import math
import torch
from torch import nn
from d2l import torch as d2l
```

## 自注意力

 给定一个由词元组成的输入序列 $\mathbf{x}_1, \ldots, \mathbf{x}_n$， 其中任意 $\mathbf{x}_i \in \mathbb{R}^d$（ $1 \leq i \leq n$ ）。 该序列的自注意力输出为一个长度相同的序列 $\mathbf{y}_1, \ldots, \mathbf{y}_n$，其中： 
$$
\mathbf{y}_i = f(\mathbf{x}_i, (\mathbf{x}_1, \mathbf{x}_1), \ldots, (\mathbf{x}_n, \mathbf{x}_n)) \in \mathbb{R}^d\tag{10.6.1}
$$
 根据 $(10.2.4)$ 中定义的注意力汇聚函数 $f$。 下面的代码片段是基于多头注意力对一个张量完成自注意力的计算， 张量的形状为（批量大小，时间步的数目或词元序列的长度，$d$）。 输出与输入的张量形状相同。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-327.png)

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-328.png)

## 位置编码

 在处理词元序列时，循环神经网络是逐个的重复地处理词元的， 而自注意力则因为并行计算而放弃了顺序操作。 为了使用序列的顺序信息，通过在输入表示中添加`位置编码`（positional encoding）来注入绝对的或相对的位置信息。 位置编码可以通过学习得到也可以直接固定得到。 

 接下来描述的是基于正弦函数和余弦函数的固定位置编码 ([Vaswani *et al.*, 2017](https://zh-v2.d2l.ai/chapter_references/zreferences.html#id174))。 

 假设输入表示 $\mathbf{X} \in \mathbb{R}^{n \times d}$ 包含一个序列中 $n$ 个词元的 $d$ 维嵌入表示。 位置编码使用相同形状的位置嵌入矩阵 $\mathbf{P} \in \mathbb{R}^{n \times d}$ 输出 $\mathbf{X} + \mathbf{P}$， 矩阵第 $i$ 行、第 $2j$ 列和 $2j+1$ 列上的元素为： 
$$
\begin{split}\begin{aligned} p_{i, 2j} &= \sin\left(\frac{i}{10000^{2j/d}}\right),\\p_{i, 2j+1} &= \cos\left(\frac{i}{10000^{2j/d}}\right).\end{aligned}\end{split}\tag{10.6.2}
$$
 让我们先在下面的`PositionalEncoding`类中实现它。 

```python
#@save
class PositionalEncoding(nn.Module):
    """位置编码"""
    def __init__(self, num_hiddens, dropout, max_len=1000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(dropout)
        # 创建一个足够长的P
        self.P = torch.zeros((1, max_len, num_hiddens))
        X = torch.arange(max_len, dtype=torch.float32).reshape(
            -1, 1) / torch.pow(10000, torch.arange(
            0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)
        self.P[:, :, 0::2] = torch.sin(X)
        self.P[:, :, 1::2] = torch.cos(X)

    def forward(self, X):
        X = X + self.P[:, :X.shape[1], :].to(X.device)
        return self.dropout(X)
```

 在位置嵌入矩阵 $\mathbf{P}$ 中， 行代表词元在序列中的位置，列代表位置编码的不同维度。 从下面的例子中可以看到位置嵌入矩阵的第6列和第7列的频率高于第8列和第9列。 第6列和第7列之间的偏移量（第8列和第9列相同）是由于正弦函数和余弦函数的交替。 

```python
encoding_dim, num_steps = 32, 60
pos_encoding = PositionalEncoding(encoding_dim, 0)
pos_encoding.eval()
X = pos_encoding(torch.zeros((1, num_steps, encoding_dim)))
P = pos_encoding.P[:, :X.shape[1], :]
d2l.plot(torch.arange(num_steps), P[0, :, 6:10].T, xlabel='Row (position)',
         figsize=(6, 2.5), legend=["Col %d" % d for d in torch.arange(6, 10)])
```

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-329.png)

### 绝对位置信息

 为了明白沿着编码维度单调降低的频率与绝对位置信息的关系， 让我们打印出 $0, 1, \ldots, 7$ 的二进制表示形式。 正如所看到的，每个数字、每两个数字和每四个数字上的比特值 在第一个最低位、第二个最低位和第三个最低位上分别交替。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-330.png)

 在二进制表示中，较高比特位的交替频率低于较低比特位， 与下面的热图所示相似，只是位置编码通过使用三角函数在编码维度上降低频率。 由于输出是浮点数，因此此类连续表示比二进制表示法更节省空间。 

```python
P = P[0, :, :].unsqueeze(0).unsqueeze(0)
d2l.show_heatmaps(P, xlabel='Column (encoding dimension)',
                  ylabel='Row (position)', figsize=(3.5, 4), cmap='Blues')
```

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-331.png)

### 相对位置信息

 除了捕获绝对位置信息之外，上述的位置编码还允许模型学习得到输入序列中相对位置信息。 这是因为对于任何确定的位置偏移 $\delta$，位置 $i + \delta$ 处 的位置编码可以线性投影位置 $i$ 处的位置编码来表示。 

 这种投影的数学解释是，令 $\omega_j = 1/10000^{2j/d}$， 对于任何确定的位置偏移 $\delta$， $(10.6.2)$ 中的任何一对 $(p_{i, 2j}, p_{i, 2j+1})$ 都可以线性投影到 $(p_{i+\delta, 2j}, p_{i+\delta, 2j+1})$： 
$$
\begin{split}\begin{aligned}&\begin{bmatrix} \cos(\delta \omega_j) & \sin(\delta \omega_j) \\  -\sin(\delta \omega_j) & \cos(\delta \omega_j) \\ \end{bmatrix}\begin{bmatrix} p_{i, 2j} \\  p_{i, 2j+1} \\ \end{bmatrix}\\=&\begin{bmatrix} \cos(\delta \omega_j) \sin(i \omega_j) + \sin(\delta \omega_j) \cos(i \omega_j) \\  -\sin(\delta \omega_j) \sin(i \omega_j) + \cos(\delta \omega_j) \cos(i \omega_j) \\ \end{bmatrix}\\=&\begin{bmatrix} \sin\left((i+\delta) \omega_j\right) \\  \cos\left((i+\delta) \omega_j\right) \\ \end{bmatrix}\\=&\begin{bmatrix} p_{i+\delta, 2j} \\  p_{i+\delta, 2j+1} \\ \end{bmatrix},\end{aligned}\end{split}\tag{10.6.3}
$$
 $2\times 2$ 投影矩阵不依赖于任何位置的索引 $i$。 

## 小结

- 在自注意力中，查询、键和值都来自同一组输入，自注意力池化层将 $x_i$当作 key，value，query来对序列抽取特征
- 为了使用序列的顺序信息，可以通过在输入表示中添加位置编码，来注入绝对的或相对的位置信息，，，使得自注意力能够记忆位置信息

# Transformer

## 模型

 Transformer作为编码器－解码器架构的一个实例，其整体架构图如下所示。正如所见到的，Transformer是由编码器和解码器组成的。

 Transformer的编码器和解码器是基于自注意力的模块叠加而成的，源（输入）序列和目标（输出）序列的`嵌入`（embedding）表示将加上`位置编码`（positional encoding），再分别输入到编码器和解码器中。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-332.png)

 从宏观角度来看，Transformer的编码器是由多个相同的层叠加而成的，每个层都有两个子层（子层表示为）。

* 第一个子层是`多头自注意力`（multi-head self-attention）汇聚；
* 第二个子层是`基于位置的前馈网络`（positionwise feed-forward network）。 

 具体来说，在计算编码器的自注意力时，查询、键和值都来自前一个编码器层的输出。受`残差网络`的启发，每个子层都采用了`残差连接`（residual connection）。 在Transformer中，对于序列中任何位置的任何输入 $\mathbf{x} \in \mathbb{R}^d$，都要求满足 $\mathrm{sublayer}(\mathbf{x}) \in \mathbb{R}^d$，以便残差连接满足 $\mathbf{x} + \mathrm{sublayer}(\mathbf{x}) \in \mathbb{R}^d$。在残差连接的加法计算之后，紧接着应用`层规范化`（layer normalization） ([Ba *et al.*, 2016](https://zh-v2.d2l.ai/chapter_references/zreferences.html#id5))。因此，输入序列对应的每个位置，Transformer编码器都将输出一个 $d$ 维表示向量。 

 Transformer解码器也是由多个相同的层叠加而成的，并且层中使用了残差连接和层规范化。除了编码器中描述的两个子层之外，解码器还在这两个子层之间插入了第三个子层，称为`编码器－解码器注意力`（encoder-decoder attention）层。

在编码器－解码器注意力中，`查询`来自前一个解码器层的输出，而`键`和`值`来自整个编码器的输出。在解码器自注意力中，`查询、键和值`都来自上一个解码器层的输出。但是，解码器中的每个位置只能考虑该位置之前的所有位置。这种`掩蔽`（masked）注意力保留了`自回归`（auto-regressive）属性，确保预测仅依赖于已生成的输出词元。 

 在此之前已经描述并实现了基于缩放点积多头注意力和位置编码。接下来将实现Transformer模型的剩余部分。 

```python
import math
import pandas as pd
import torch
from torch import nn
from d2l import torch as d2l
```

## 基于位置的前馈网络

 基于位置的前馈网络对序列中的所有位置的表示进行变换时使用的是同一个多层感知机（MLP），这就是称前馈网络是`基于位置的`（positionwise）的原因。

* 将输入形状由 $(b,n,d)$ 变换成 $(bn,d)$
* 作用两个全连接层
* 输出形状由 $(bn,d)$ 变化回 $(b,n,d)$

在下面的实现中，输入`X`的形状（批量大小，时间步数或序列长度，隐单元数或特征维度）将被一个两层的感知机转换成形状为（批量大小，时间步数，`ffn_num_outputs`）的输出张量。 

```python
#@save
class PositionWiseFFN(nn.Module):
    """基于位置的前馈网络"""
    def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs,
                 **kwargs):
        super(PositionWiseFFN, self).__init__(**kwargs)
        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)
        self.relu = nn.ReLU()
        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)

    def forward(self, X):
        return self.dense2(self.relu(self.dense1(X)))
```

 下面的例子显示，改变张量的最里层维度的尺寸，会改变成基于位置的前馈网络的输出尺寸。因为用同一个多层感知机对所有位置上的输入进行变换，所以当所有这些位置的输入相同时，它们的输出也是相同的。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-333.png)

## 残差连接和层规范化

 现在让我们关注`加法和规范化`（add&norm）组件。正如在本节开头所述，这是由残差连接和紧随其后的层规范化组成的。两者都是构建有效的深度架构的关键。 

> 层归一化

* 批量归一化对每个特征/通道里元素进行归一化（不适合序列长度会变的NLP应用）
* 层归一化对每个样本里的元素进行归一化

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-334.png)

 现在可以使用残差连接和层规范化来实现`AddNorm`类。暂退法也被作为正则化方法使用。 

```python
#@save
class AddNorm(nn.Module):
    """残差连接后进行层规范化"""
    def __init__(self, normalized_shape, dropout, **kwargs):
        super(AddNorm, self).__init__(**kwargs)
        self.dropout = nn.Dropout(dropout)
        self.ln = nn.LayerNorm(normalized_shape)

    def forward(self, X, Y):
        return self.ln(self.dropout(Y) + X)
```

 残差连接要求两个输入的形状相同，以便加法操作后输出张量的形状相同。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-335.png)

## 信息传递

* 编码器中的输出 $\mathbf{y_1}, \ldots,\mathbf{y_n} $

* 将其作为解码中第 $i$ 个 transformer块中多头注意力的key和value（它的query来自目标序列）
* 意味着编码器和解码器中块的个数和输出维度一样

## 编码器

 有了组成Transformer编码器的基础组件，现在可以先实现编码器中的一个层。

下面的`EncoderBlock`类包含两个子层：多头自注意力和基于位置的前馈网络，这两个子层都使用了残差连接和紧随的层规范化。 

```python
#@save
class EncoderBlock(nn.Module):
    """Transformer编码器块"""
    def __init__(self, key_size, query_size, value_size, num_hiddens,
                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,
                 dropout, use_bias=False, **kwargs):
        super(EncoderBlock, self).__init__(**kwargs)
        self.attention = d2l.MultiHeadAttention(
            key_size, query_size, value_size, num_hiddens, num_heads, dropout,
            use_bias)
        self.addnorm1 = AddNorm(norm_shape, dropout)
        self.ffn = PositionWiseFFN(
            ffn_num_input, ffn_num_hiddens, num_hiddens)
        self.addnorm2 = AddNorm(norm_shape, dropout)

    def forward(self, X, valid_lens):
        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))
        return self.addnorm2(Y, self.ffn(Y))
```

 正如从代码中所看到的，Transformer编码器中的任何层都不会改变其输入的形状。 

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-336.png)

 下面实现的Transformer编码器的代码中，堆叠了`num_layers`个`EncoderBlock`类的实例。由于这里使用的是值范围在-1和1之间的固定位置编码，因此通过学习得到的输入的嵌入表示的值需要先乘以嵌入维度的平方根进行重新缩放，然后再与位置编码相加。 

```python
#@save
class TransformerEncoder(d2l.Encoder):
    """Transformer编码器"""
    def __init__(self, vocab_size, key_size, query_size, value_size,
                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,
                 num_heads, num_layers, dropout, use_bias=False, **kwargs):
        super(TransformerEncoder, self).__init__(**kwargs)
        self.num_hiddens = num_hiddens
        self.embedding = nn.Embedding(vocab_size, num_hiddens)
        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)
        self.blks = nn.Sequential()
        for i in range(num_layers):
            self.blks.add_module("block"+str(i),
                EncoderBlock(key_size, query_size, value_size, num_hiddens,
                             norm_shape, ffn_num_input, ffn_num_hiddens,
                             num_heads, dropout, use_bias))

    def forward(self, X, valid_lens, *args):
        # 因为位置编码值在-1和1之间，
        # 因此嵌入值乘以嵌入维度的平方根进行缩放，
        # 然后再与位置编码相加。
        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))
        self.attention_weights = [None] * len(self.blks)
        for i, blk in enumerate(self.blks):
            X = blk(X, valid_lens)
            self.attention_weights[
                i] = blk.attention.attention.attention_weights
        return X
```

 下面我们指定了超参数来创建一个两层的Transformer编码器。 Transformer编码器输出的形状是（批量大小，时间步数目，`num_hiddens`） 

```python
encoder = TransformerEncoder(
    200, 24, 24, 24, 24, [100, 24], 24, 48, 8, 2, 0.5)
encoder.eval()
encoder(torch.ones((2, 100), dtype=torch.long), valid_lens).shape
```

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-337.png)

## 解码器

 Transformer解码器也是由多个相同的层组成，在`DecoderBlock`类中实现的每个层包含了三个子层：解码器自注意力、“编码器-解码器”注意力和基于位置的前馈网络。这些子层也都被残差连接和紧随的层规范化围绕。 

 正如在本节前面所述，在掩蔽多头解码器自注意力层（第一个子层）中，查询、键和值都来自上一个解码器层的输出。

关于`序列到序列模型`（sequence-to-sequence model），在训练阶段，其输出序列的所有位置（时间步）的词元都是已知的；然而，在预测阶段，其输出序列的词元是逐个生成的。因此，在任何解码器时间步中，只有生成的词元才能用于解码器的自注意力计算中。

为了在解码器中保留自回归的属性，其掩蔽自注意力设定了参数`dec_valid_lens`，以便任何查询都只会与解码器中所有已经生成词元的位置（即直到该查询位置为止）进行注意力计算。 

```python
class DecoderBlock(nn.Module):
    """解码器中第i个块"""
    def __init__(self, key_size, query_size, value_size, num_hiddens,
                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,
                 dropout, i, **kwargs):
        super(DecoderBlock, self).__init__(**kwargs)
        self.i = i
        self.attention1 = d2l.MultiHeadAttention(
            key_size, query_size, value_size, num_hiddens, num_heads, dropout)
        self.addnorm1 = AddNorm(norm_shape, dropout)
        self.attention2 = d2l.MultiHeadAttention(
            key_size, query_size, value_size, num_hiddens, num_heads, dropout)
        self.addnorm2 = AddNorm(norm_shape, dropout)
        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens,
                                   num_hiddens)
        self.addnorm3 = AddNorm(norm_shape, dropout)

    def forward(self, X, state):
        enc_outputs, enc_valid_lens = state[0], state[1]
        # 训练阶段，输出序列的所有词元都在同一时间处理，
        # 因此state[2][self.i]初始化为None。
        # 预测阶段，输出序列是通过词元一个接着一个解码的，
        # 因此state[2][self.i]包含着直到当前时间步第i个块解码的输出表示
        if state[2][self.i] is None:
            key_values = X
        else:
            key_values = torch.cat((state[2][self.i], X), axis=1)
        state[2][self.i] = key_values
        if self.training:
            batch_size, num_steps, _ = X.shape
            # dec_valid_lens的开头:(batch_size,num_steps),
            # 其中每一行是[1,2,...,num_steps]
            dec_valid_lens = torch.arange(
                1, num_steps + 1, device=X.device).repeat(batch_size, 1)
        else:
            dec_valid_lens = None

        # 自注意力
        X2 = self.attention1(X, key_values, key_values, dec_valid_lens)
        Y = self.addnorm1(X, X2)
        # 编码器－解码器注意力。
        # enc_outputs的开头:(batch_size,num_steps,num_hiddens)
        Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)
        Z = self.addnorm2(Y, Y2)
        return self.addnorm3(Z, self.ffn(Z)), state
```

 为了便于在“编码器－解码器”注意力中进行缩放点积计算和残差连接中进行加法计算，编码器和解码器的特征维度都是`num_hiddens`。 

```python
decoder_blk = DecoderBlock(24, 24, 24, 24, [100, 24], 24, 48, 8, 0.5, 0)
decoder_blk.eval()
X = torch.ones((2, 100, 24))
state = [encoder_blk(X, valid_lens), valid_lens, [None]]
decoder_blk(X, state)[0].shape
```

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-338.png)

现在我们构建了由`num_layers`个`DecoderBlock`实例组成的完整的Transformer解码器。最后，通过一个全连接层计算所有`vocab_size`个可能的输出词元的预测值。解码器的自注意力权重和编码器解码器注意力权重都被存储下来，方便日后可视化的需要。 

```python
class TransformerDecoder(d2l.AttentionDecoder):
    def __init__(self, vocab_size, key_size, query_size, value_size,
                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,
                 num_heads, num_layers, dropout, **kwargs):
        super(TransformerDecoder, self).__init__(**kwargs)
        self.num_hiddens = num_hiddens
        self.num_layers = num_layers
        self.embedding = nn.Embedding(vocab_size, num_hiddens)
        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)
        self.blks = nn.Sequential()
        for i in range(num_layers):
            self.blks.add_module("block"+str(i),
                DecoderBlock(key_size, query_size, value_size, num_hiddens,
                             norm_shape, ffn_num_input, ffn_num_hiddens,
                             num_heads, dropout, i))
        self.dense = nn.Linear(num_hiddens, vocab_size)

    def init_state(self, enc_outputs, enc_valid_lens, *args):
        return [enc_outputs, enc_valid_lens, [None] * self.num_layers]

    def forward(self, X, state):
        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))
        self._attention_weights = [[None] * len(self.blks) for _ in range (2)]
        for i, blk in enumerate(self.blks):
            X, state = blk(X, state)
            # 解码器自注意力权重
            self._attention_weights[0][
                i] = blk.attention1.attention.attention_weights
            # “编码器－解码器”自注意力权重
            self._attention_weights[1][
                i] = blk.attention2.attention.attention_weights
        return self.dense(X), state

    @property
    def attention_weights(self):
        return self._attention_weights
```

## 训练

 依照Transformer架构来实例化编码器－解码器模型。在这里，指定Transformer的编码器和解码器都是2层，都使用4头注意力 

 下面在“英语－法语”机器翻译数据集上训练Transformer模型。 

```python
num_hiddens, num_layers, dropout, batch_size, num_steps = 32, 2, 0.1, 64, 10
lr, num_epochs, device = 0.005, 200, d2l.try_gpu()
ffn_num_input, ffn_num_hiddens, num_heads = 32, 64, 4
key_size, query_size, value_size = 32, 32, 32
norm_shape = [32]

train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)

encoder = TransformerEncoder(
    len(src_vocab), key_size, query_size, value_size, num_hiddens,
    norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,
    num_layers, dropout)
decoder = TransformerDecoder(
    len(tgt_vocab), key_size, query_size, value_size, num_hiddens,
    norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,
    num_layers, dropout)
net = d2l.EncoderDecoder(encoder, decoder)
d2l.train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)
```

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-339.png)

 训练结束后，使用Transformer模型将一些英语句子翻译成法语，并且计算它们的BLEU分数。 

```python
engs = ['go .', "i lost .", 'he\'s calm .', 'i\'m home .']
fras = ['va !', 'j\'ai perdu .', 'il est calme .', 'je suis chez moi .']
for eng, fra in zip(engs, fras):
    translation, dec_attention_weight_seq = d2l.predict_seq2seq(
        net, eng, src_vocab, tgt_vocab, num_steps, device, True)
    print(f'{eng} => {translation}, ',
          f'bleu {d2l.bleu(translation, fra, k=2):.3f}')
```

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-340.png)

 当进行最后一个英语到法语的句子翻译工作时，让我们可视化Transformer的注意力权重。编码器自注意力权重的形状为（编码器层数，注意力头数，`num_steps`或查询的数目，`num_steps`或“键－值”对的数目）。 

```python
enc_attention_weights = torch.cat(net.encoder.attention_weights, 0).reshape((num_layers, num_heads,
    -1, num_steps))
enc_attention_weights.shape
```

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-341.png)

 在编码器的自注意力中，查询和键都来自相同的输入序列。因为填充词元是不携带信息的，因此通过指定输入序列的有效长度可以避免查询与使用填充词元的位置计算注意力。接下来，将逐行呈现两层多头注意力的权重。每个注意力头都根据查询、键和值的不同的表示子空间来表示不同的注意力。 

```python
d2l.show_heatmaps(
    enc_attention_weights.cpu(), xlabel='Key positions',
    ylabel='Query positions', titles=['Head %d' % i for i in range(1, 5)],
    figsize=(7, 3.5))
```

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-342.png)

 为了可视化解码器的自注意力权重和“编码器－解码器”的注意力权重，我们需要完成更多的数据操作工作。例如用零填充被掩蔽住的注意力权重。值得注意的是，解码器的自注意力权重和“编码器－解码器”的注意力权重都有相同的查询：即以`序列开始词元`（beginning-of-sequence,BOS）打头，再与后续输出的词元共同组成序列。 

```python
dec_attention_weights_2d = [head[0].tolist()
                            for step in dec_attention_weight_seq
                            for attn in step for blk in attn for head in blk]
dec_attention_weights_filled = torch.tensor(
    pd.DataFrame(dec_attention_weights_2d).fillna(0.0).values)
dec_attention_weights = dec_attention_weights_filled.reshape((-1, 2, num_layers, num_heads, num_steps))
dec_self_attention_weights, dec_inter_attention_weights = \
    dec_attention_weights.permute(1, 2, 3, 0, 4)
dec_self_attention_weights.shape, dec_inter_attention_weights.shape
```

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-343.png)

 由于解码器自注意力的自回归属性，查询不会对当前位置之后的“键－值”对进行注意力计算 

```python
# Plusonetoincludethebeginning-of-sequencetoken
d2l.show_heatmaps(
    dec_self_attention_weights[:, :, :, :len(translation.split()) + 1],
    xlabel='Key positions', ylabel='Query positions',
    titles=['Head %d' % i for i in range(1, 5)], figsize=(7, 3.5))
```

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-344.png)

 与编码器的自注意力的情况类似，通过指定输入序列的有效长度，输出序列的查询不会与输入序列中填充位置的词元进行注意力计算。 

```python
d2l.show_heatmaps(
    dec_inter_attention_weights, xlabel='Key positions',
    ylabel='Query positions', titles=['Head %d' % i for i in range(1, 5)],
    figsize=(7, 3.5))
```

![](https://cyan-images.oss-cn-shanghai.aliyuncs.com/images/deep-learning-20230716-345.png)

## 小结

- Transformer是编码器－解码器架构的一个实践，尽管在实际情况中编码器或解码器可以单独使用。
- 在Transformer中，多头自注意力用于表示输入序列和输出序列，不过解码器必须通过掩蔽机制来保留自回归属性。
- Transformer中的残差连接和层规范化是训练非常深度模型的重要工具。
- Transformer模型中基于位置的前馈网络使用同一个多层感知机，作用是对所有序列位置的表示进行转换。